{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy,minisom,scikit-learn,tqdm,SimpleITK,matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from minisom import MiniSom\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def som_silhouette(som, data):\n",
    "    # Assign data points to clusters based on BMU\n",
    "    labels = np.array([som.winner(x)[0] for x in data])\n",
    "\n",
    "    # Calculate Silhouette Coefficient\n",
    "    silhouette_avg = silhouette_score(data, labels)\n",
    "    #print('avg_silhoutte',silhouette_avg)\n",
    "    return silhouette_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minisom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_six_color=np.array([\n",
    "    [0.764, 0.863, 0.933],  # Light blue (calming, airy)\n",
    "    [0.294, 0.533, 0.851],  # Royal blue (trustworthy, professional)\n",
    "    [0.988, 0.737, 0.831],  # Light pink (soft, romantic)\n",
    "    [0.937, 0.831, 0.706],  # Light brown (natural, earthy)\n",
    "    [0.439, 0.859, 0.392],  # Forest green (refreshing, serene)\n",
    "    [0.996, 0.878, 0.631],  # Peach (warm, cheerful)\n",
    "], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import os,time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from minisom import MiniSom\n",
    "from sklearn.metrics import silhouette_score\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from minisom import MiniSom\n",
    "import pandas as pd\n",
    "\n",
    "fts=[]\n",
    "gb_avs=[]\n",
    "pat=[]\n",
    "#C://Users//Administrator//Desktop//gb_voxel_outputs//20_mm/TMH/gldm/120433028/\n",
    "#Mainpath='C://Users//Administrator//Desktop//gb_voxel_outputs//20_mm/TMH/gldm/'\n",
    "Mainpath='C://Users//Administrator//Desktop/GB_ANALYSIS/GB_ANALYSIS_new/20_mm_final/TMH/gldm/'\n",
    "for gb in tqdm.tqdm(os.listdir(Mainpath)):\n",
    "    pat.append(gb)\n",
    "    fts1=[]\n",
    "    gb_avs1=[]\n",
    "    Pathnrrd=Mainpath+gb+\"/\"\n",
    "    ##print(Pathnrrd)\n",
    "    for dirName, subdirList, fileList in(os.walk(Pathnrrd)):\n",
    "           # print(dirName)\n",
    "            for filename in fileList:\n",
    "                #print(filename)\n",
    "\n",
    "                start_time = datetime.now()\n",
    "                print(start_time)\n",
    "                print(\"Starting operation...\")\n",
    "                if \".nrrd\" in filename.lower():\n",
    "                        print(time.time)\n",
    "                        avs=[]\n",
    "                        ##print('_____________________________________________________________________________________')\n",
    "                        #c+=1\n",
    "                        print(filename)\n",
    "                        fts1.append(filename)\n",
    "                        ##print('full file path\\n',os.path.join(dirName, filename))\n",
    "                        #ds = pydicom.filereader.dcmread(os.path.join(dirName, filename))\n",
    "                        #image = sitk.ReadImage('C://Users//Administrator//Desktop//gb_voxel_outputs//20_mm/lung1/gldm/LUNG1-010/LUNG1-010_original_gldm_DependenceNonUniformity.nrrd')\n",
    "                        image = sitk.ReadImage(os.path.join(dirName, filename))\n",
    "                        image_array = sitk.GetArrayFromImage(image)\n",
    "\n",
    "\n",
    "\n",
    "                        for i in range(1,image_array.shape[0]-1):\n",
    "                            ##print('_____________________________________________________________________________________________________________')\n",
    "                            ##print('shape:',image_array.shape)\n",
    "                            # Check the image dimensions (assuming the image is in shape (depth, height, width, channels) or (depth, height, width))\n",
    "                            if image_array.ndim not in (3, 4):\n",
    "                                raise ValueError(\"Image array must be a 3D or 4D array\")\n",
    "\n",
    "                            # If the image has more than one channel, assume the last dimension is the channel dimension\n",
    "                            if image_array.ndim == 3:\n",
    "                                # Select a specific slice for 2D segmentation\n",
    "                                slice_index = i # Adjust the slice index as needed\n",
    "                                if slice_index >= image_array.shape[0]:\n",
    "                                    raise ValueError(f\"Invalid slice index. Maximum index allowed: {image_array.shape[0] - 1}\")\n",
    "                                image_slice = image_array[slice_index]\n",
    "                            else:\n",
    "                                # The image is already 3D, select a default slice\n",
    "                                image_slice = image_array[image_array.shape[0] // 2]\n",
    "\n",
    "                            # Normalize the image slice\n",
    "                            image_slice = image_slice.astype(np.float32) / np.max(image_slice)\n",
    "\n",
    "                            # Ensure the slice is 2D for display and processing\n",
    "                            if image_slice.ndim == 3 and image_slice.shape[2] not in (1, 3):\n",
    "                                # If it's neither single-channel nor RGB, pick the first channel\n",
    "                                image_slice = image_slice[:, :, 0]\n",
    "\n",
    "                            # Reshape the image to be a list of pixels\n",
    "                            if image_slice.ndim == 3:\n",
    "                                # For multi-channel images\n",
    "                                pixels = image_slice.reshape(-1, image_slice.shape[2])\n",
    "                                input_len = image_slice.shape[2]\n",
    "                            else:\n",
    "                                # For grayscale images\n",
    "                                pixels = image_slice.reshape(-1, 1)\n",
    "                                input_len = 1\n",
    "\n",
    "                            # Define the SOM\n",
    "                            som_width =2\n",
    "                            som_height = 2\n",
    "                            som = MiniSom(x=som_width, y=som_height, input_len=input_len, sigma=1.0, learning_rate=0.8)\n",
    "                            som.random_weights_init(pixels)\n",
    "\n",
    "                            # Train the SOM\n",
    "                            ##print(\"Training the SOM...\")\n",
    "                            ##print(i)\n",
    "                            #som.train_random(pixels, 10000,verbose= True)\n",
    "                            som.train(pixels, 1000,verbose= False)\n",
    "\n",
    "                            st=som_silhouette(som=som, data=pixels)\n",
    "                            avs.append(st)\n",
    "\n",
    "                            # Map each pixel to the SOM and assign a label\n",
    "                            ##print(\"Segmenting the image...\")\n",
    "                            labels = np.zeros((image_slice.shape[0], image_slice.shape[1]), dtype=int)\n",
    "                            ##print('Number of labels',len(labels))\n",
    "                            for i in range(image_slice.shape[0]):\n",
    "                                for j in range(image_slice.shape[1]):\n",
    "                                    pixel = image_slice[i, j] if input_len > 1 else [image_slice[i, j]]\n",
    "                                    w = som.winner(pixel)\n",
    "                                    labels[i, j] = w[0] * som_width + w[1]\n",
    "\n",
    "                            # Map the labels to colors\n",
    "                            #label_colors = np.random.rand(som_width * som_height, 3)\n",
    "                            label_colors = np_six_color\n",
    "                            #label_colors=np_six_color\n",
    "                            segmented_image = label_colors[labels]\n",
    "\n",
    "                            # Rescale segmented image to [0, 255] for better display\n",
    "                            segmented_image = (segmented_image * 255).astype(np.uint8)\n",
    "\n",
    "    #                         # Plot the original and segmented images side by side\n",
    "    #                         fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    #                         # Original image\n",
    "    #                         if image_slice.ndim == 2:\n",
    "    #                             axes[0].imshow(image_slice, cmap='bone')\n",
    "    #                         else:\n",
    "    #                             axes[0].imshow(image_slice)\n",
    "    #                         axes[0].set_title(f'Input Image (Slice {slice_index})')\n",
    "    #                         axes[0].axis('off')\n",
    "\n",
    "    #                         #print('\\t\\t\\t',re.split('/',nrrd_path)[-1])\n",
    "    #                         print('\\t\\t\\t',filename)\n",
    "    #                         # Segmented image\n",
    "    #                         axes[1].imshow(segmented_image)\n",
    "    #                         #axes[1].set_title('Segmented Image',som_width,' X ',som_height, 'grid')\n",
    "    #                         axes[1].set_title(f'Segmented Image ({som_width}x{som_height} grid)')\n",
    "\n",
    "    #                         axes[1].axis('off')\n",
    "\n",
    "    #                         plt.show()\n",
    "                        \n",
    "                        print('Average Silhoutte score:',np.sum(np.array(avs))/len(avs))\n",
    "                        print('________________________________________________________')\n",
    "                        gb_avs1.append(np.sum(np.array(avs))/len(avs))\n",
    "\n",
    "    fts.append(fts1)\n",
    "    gb_avs.append(gb_avs1)            \n",
    "end_time = datetime.now()\n",
    "\n",
    "# Calculate the duration\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(duration)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "167*14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "final_silhoutte=pd.DataFrame(gb_avs,columns=[fts[0]],index=pat)\n",
    "final_silhoutte.to_csv('TMH_average_silhoutte_score_new')\n",
    "print('done saving to csv')\n",
    "final_silhoutte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from minisom import MiniSom\n",
    "\n",
    "# Load the NRRD image using SimpleITK\n",
    "#nrrd_path = 'C://Users//Administrator//Desktop//gb_voxel_outputs//20_mm/TMH/gldm/110348284/110348284_original_gldm_HighGrayLevelEmphasis.nrrd'  # Replace with your NRRD image path\n",
    "nrrd_path =\"C://Users//Administrator//Desktop//GB_ANALYSIS//GB_ANALYSIS_new//20_mm_final//TMH//gldm/105131012/105131012_original_gldm_HighGrayLevelEmphasis.nrrd\"\n",
    "image = sitk.ReadImage(nrrd_path)\n",
    "image_array = sitk.GetArrayFromImage(image)\n",
    "\n",
    "# Check the image dimensions (assuming the image is in shape (depth, height, width, channels) or (depth, height, width))\n",
    "if image_array.ndim not in (3, 4):\n",
    "    raise ValueError(\"Image array must be a 3D or 4D array\")\n",
    "\n",
    "# If the image has more than one channel, assume the last dimension is the channel dimension\n",
    "if image_array.ndim == 3:\n",
    "    # Ensure we have enough slices to plot\n",
    "    num_slices = min(10, image_array.shape[0])\n",
    "else:\n",
    "    num_slices = 1\n",
    "\n",
    "# Normalize the image slice\n",
    "def normalize_slice(slice):\n",
    "    return (slice.astype(np.float32) / np.max(slice)).clip(0, 1)\n",
    "\n",
    "# Define the SOM parameters\n",
    "som_width = 2\n",
    "som_height =2\n",
    "input_len = image_array.shape[-1] if image_array.ndim == 4 else 1\n",
    "som = MiniSom(x=som_width, y=som_height, input_len=input_len, sigma=1.0, learning_rate=0.5)\n",
    "\n",
    "#label_colors = np.random.rand(som_width * som_height, 3)\n",
    "\n",
    "# Create a figure with subplots for each slice\n",
    "fig, axes = plt.subplots(10, 2, figsize=(12, 30))\n",
    "\n",
    "for i in range(num_slices):\n",
    "    # Select the slice\n",
    "    image_slice = image_array[i]\n",
    "\n",
    "    # Normalize the image slice\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "\n",
    "    # Reshape the image to be a list of pixels\n",
    "    pixels = image_slice_norm.reshape(-1, input_len)\n",
    "\n",
    "    # Initialize and train the SOM\n",
    "    som.random_weights_init(pixels)\n",
    "    som.train_random(pixels, 1000)\n",
    "\n",
    "    # Map each pixel to the SOM and assign a label\n",
    "    labels = np.zeros((image_slice.shape[0], image_slice.shape[1]), dtype=int)\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            w = som.winner(image_slice_norm[x, y])\n",
    "            labels[x, y] = w[0] * som_width + w[1]\n",
    "\n",
    "    # Map the labels to colors\n",
    "    #label_colors = np.random.rand(som_width * som_height, 3,s)\n",
    "    segmented_image = label_colors[labels]\n",
    "\n",
    "    # Rescale segmented image to [0, 255] for better display\n",
    "    segmented_image = (segmented_image * 255).astype(np.uint8)\n",
    "\n",
    "    # Plot the original and segmented images\n",
    "    axes[i, 0].imshow(image_slice_norm, cmap='gray' if input_len == 1 else None)\n",
    "    axes[i, 0].set_title(f'Original Image (Slice {i})')\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(segmented_image)\n",
    "    axes[i, 1].set_title(f'Segmented Image (Slice {i})')\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from minisom import MiniSom\n",
    "\n",
    "# Load the NRRD image using SimpleITK\n",
    "nrrd_path = \"C://Users//Administrator//Desktop//GB_ANALYSIS//GB_ANALYSIS_new//20_mm_final//TMH//gldm/105131012/105131012_original_gldm_HighGrayLevelEmphasis.nrrd\"\n",
    "image = sitk.ReadImage(nrrd_path)\n",
    "image_array = sitk.GetArrayFromImage(image)\n",
    "\n",
    "# Check the image dimensions\n",
    "if image_array.ndim not in (3, 4):\n",
    "    raise ValueError(\"Image array must be a 3D or 4D array\")\n",
    "\n",
    "# Determine the number of slices\n",
    "num_slices = min(10, image_array.shape[0]) if image_array.ndim == 3 else 1\n",
    "\n",
    "# Normalize the image slice\n",
    "def normalize_slice(slice):\n",
    "    return (slice.astype(np.float32) / np.max(slice)).clip(0, 1)\n",
    "\n",
    "# Define the SOM parameters\n",
    "som_width = 2\n",
    "som_height = 2\n",
    "input_len = image_array.shape[-1] if image_array.ndim == 4 else 1\n",
    "som = MiniSom(x=som_width, y=som_height, input_len=input_len, sigma=1.0, learning_rate=0.5)\n",
    "\n",
    "# Create a figure with subplots for each slice\n",
    "fig, axes = plt.subplots(num_slices, 2, figsize=(12, 30))\n",
    "\n",
    "for i in range(num_slices):\n",
    "    # Select the slice\n",
    "    image_slice = image_array[i]\n",
    "\n",
    "    # Normalize the image slice\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "\n",
    "    # Reshape the image to be a list of pixels\n",
    "    pixels = image_slice_norm.reshape(-1, input_len)\n",
    "\n",
    "    # Train the SOM without random initialization\n",
    "    som.train_random(pixels, 1000)\n",
    "\n",
    "    # Map each pixel to the SOM and assign a label\n",
    "    labels = np.zeros((image_slice.shape[0], image_slice.shape[1]), dtype=int)\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            w = som.winner(image_slice_norm[x, y])\n",
    "            labels[x, y] = w[0] * som_width + w[1]\n",
    "\n",
    "    # Map the labels to colors\n",
    "    label_colors = np.random.rand(som_width * som_height, 3)  # Ensure to generate colors here\n",
    "    segmented_image = label_colors[labels]\n",
    "\n",
    "    # Rescale segmented image to [0, 255] for better display\n",
    "    segmented_image = (segmented_image * 255).astype(np.uint8)\n",
    "\n",
    "    # Plot the original and segmented images\n",
    "    axes[i, 0].imshow(image_slice_norm, cmap='gray' if input_len == 1 else None)\n",
    "    axes[i, 0].set_title(f'Original Image (Slice {i})')\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(segmented_image)\n",
    "    axes[i, 1].set_title(f'Segmented Image (Slice {i})')\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from minisom import MiniSom\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the NRRD image using SimpleITK\n",
    "nrrd_path = \"C://Users//Administrator//Desktop//GB_ANALYSIS//GB_ANALYSIS_new//20_mm_final//TMH//gldm/105131012/105131012_original_gldm_HighGrayLevelEmphasis.nrrd\"\n",
    "image = sitk.ReadImage(nrrd_path)\n",
    "image_array = sitk.GetArrayFromImage(image)\n",
    "\n",
    "# Check image dimensions\n",
    "if image_array.ndim not in (3, 4):\n",
    "    raise ValueError(\"Image array must be a 3D or 4D array\")\n",
    "\n",
    "if image_array.ndim == 3:\n",
    "    num_slices = min(10, image_array.shape[0])\n",
    "else:\n",
    "    num_slices = 1\n",
    "\n",
    "def normalize_slice(slice):\n",
    "    return (slice.astype(np.float32) / np.max(slice)).clip(0, 1)\n",
    "\n",
    "# Define SOM parameters\n",
    "som_width = 2\n",
    "som_height = 2\n",
    "input_len = image_array.shape[-1] if image_array.ndim == 4 else 1\n",
    "som = MiniSom(x=som_width, y=som_height, input_len=input_len, sigma=1.0, learning_rate=0.5)\n",
    "\n",
    "# Create a DataFrame to hold features\n",
    "features = []\n",
    "\n",
    "for i in range(num_slices):\n",
    "    # Select the slice\n",
    "    image_slice = image_array[i]\n",
    "    \n",
    "    # Normalize the image slice\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "    \n",
    "    # Reshape image to list of pixels\n",
    "    pixels = image_slice_norm.reshape(-1, input_len)\n",
    "\n",
    "    # Train the SOM\n",
    "    som.random_weights_init(pixels)\n",
    "    som.train_random(pixels, 1000)\n",
    "\n",
    "    # Map each pixel to the SOM and assign a label\n",
    "    labels = np.zeros((image_slice.shape[0], image_slice.shape[1]), dtype=int)\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            w = som.winner(image_slice_norm[x, y])\n",
    "            labels[x, y] = w[0] * som_width + w[1]\n",
    "\n",
    "    # Extract features from each cluster\n",
    "    for cluster in range(som_width * som_height):\n",
    "        cluster_pixels = pixels[labels.flatten() == cluster]\n",
    "        \n",
    "        if cluster_pixels.size > 0:\n",
    "            mean_feature = np.mean(cluster_pixels, axis=0)\n",
    "            std_feature = np.std(cluster_pixels, axis=0)\n",
    "            features.append(np.concatenate([[i, cluster], mean_feature, std_feature]))  # i: slice index, cluster: cluster label\n",
    "\n",
    "# Convert features to DataFrame\n",
    "feature_array = np.array(features)\n",
    "columns = ['Slice', 'Cluster'] + [f'Feature_{j}' for j in range(input_len)] + [f'Std_Feature_{j}' for j in range(input_len)]\n",
    "features_df = pd.DataFrame(feature_array, columns=columns)\n",
    "\n",
    "# Standardize features if necessary\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features_df.iloc[:, 2:].astype(float))\n",
    "scaled_features_df = pd.DataFrame(scaled_features, columns=features_df.columns[2:])\n",
    "\n",
    "# Now, scaled_features_df can be used as input to the Cox model\n",
    "print(scaled_features_df)\n",
    "\n",
    "# Continue with the Cox model implementation using the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "845+1075+575+1424+632+684\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from minisom import MiniSom\n",
    "\n",
    "# Load the NRRD image using SimpleITK\n",
    "nrrd_path = \"C://Users//Administrator//Desktop//GB_ANALYSIS//GB_ANALYSIS_new//20_mm_final//TMH//gldm/105131012/105131012_original_gldm_HighGrayLevelEmphasis.nrrd\"\n",
    "image = sitk.ReadImage(nrrd_path)\n",
    "image_array = sitk.GetArrayFromImage(image)\n",
    "\n",
    "# Check image dimensions\n",
    "if image_array.ndim not in (3, 4):\n",
    "    raise ValueError(\"Image array must be a 3D or 4D array\")\n",
    "\n",
    "if image_array.ndim == 3:\n",
    "    num_slices = min(10, image_array.shape[0])\n",
    "else:\n",
    "    num_slices = 1\n",
    "\n",
    "def normalize_slice(slice):\n",
    "    return (slice.astype(np.float32) / np.max(slice)).clip(0, 1)\n",
    "\n",
    "# Define SOM parameters\n",
    "som_width = 2\n",
    "som_height = 2\n",
    "input_len = image_array.shape[-1] if image_array.ndim == 4 else 1\n",
    "som = MiniSom(x=som_width, y=som_height, input_len=input_len, sigma=1.0, learning_rate=0.5)\n",
    "\n",
    "# Dictionary to hold pixels grouped by cluster labels\n",
    "pixel_clusters = {label: [] for label in range(som_width * som_height)}\n",
    "\n",
    "for i in range(num_slices):\n",
    "    # Select the slice\n",
    "    image_slice = image_array[i]\n",
    "    \n",
    "    # Normalize the image slice\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "    \n",
    "    # Reshape image to list of pixels\n",
    "    pixels = image_slice_norm.reshape(-1, input_len)\n",
    "\n",
    "    # Train the SOM\n",
    "    som.random_weights_init(pixels)\n",
    "    som.train_random(pixels, 1000)\n",
    "\n",
    "    # Map each pixel to the SOM and assign a label\n",
    "    labels = np.zeros((image_slice.shape[0], image_slice.shape[1]), dtype=int)\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            w = som.winner(image_slice_norm[x, y])\n",
    "            labels[x, y] = w[0] * som_width + w[1]\n",
    "\n",
    "    # Extract pixel values into lists grouped by cluster label\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            cluster_label = labels[x, y]\n",
    "            pixel_value = image_slice[x, y]  # Use original pixel value (before normalization)\n",
    "            pixel_clusters[cluster_label].append(pixel_value)\n",
    "\n",
    "# Plot the pixel value distributions for each cluster\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for label, pixels in pixel_clusters.items():\n",
    "    plt.hist(pixels, bins=30, alpha=0.6, label=f'Cluster {label}', density=True)\n",
    "\n",
    "plt.title('Pixel Value Distribution by Cluster')\n",
    "plt.xlabel('Pixel Value')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from minisom import MiniSom\n",
    "\n",
    "# Load the NRRD image using SimpleITK\n",
    "nrrd_path = \"C://Users//Administrator//Desktop//GB_ANALYSIS//GB_ANALYSIS_new//20_mm_final//TMH//gldm/105131012/105131012_original_gldm_SmallDependenceLowGrayLevelEmphasis.nrrd\"\n",
    "image = sitk.ReadImage(nrrd_path)\n",
    "image_array = sitk.GetArrayFromImage(image)\n",
    "\n",
    "# Check image dimensions\n",
    "if image_array.ndim not in (3, 4):\n",
    "    raise ValueError(\"Image array must be a 3D or 4D array\")\n",
    "\n",
    "if image_array.ndim == 3:\n",
    "    num_slices = min(10, image_array.shape[0])\n",
    "else:\n",
    "    num_slices = 1\n",
    "\n",
    "def normalize_slice(slice):\n",
    "    max_val = np.max(slice)\n",
    "    if max_val > 0:\n",
    "        return (slice.astype(np.float32) / max_val).clip(0, 1)\n",
    "    else:\n",
    "        return np.zeros_like(slice, dtype=np.float32)  # Return an array of zeros if max is zero\n",
    "\n",
    "# Define SOM parameters\n",
    "som_width = 2\n",
    "som_height = 2\n",
    "input_len = image_array.shape[-1] if image_array.ndim == 4 else 1\n",
    "som = MiniSom(x=som_width, y=som_height, input_len=input_len, sigma=1.0, learning_rate=0.5)\n",
    "\n",
    "# Dictionary to hold pixels grouped by cluster labels\n",
    "pixel_clusters = {label: [] for label in range(som_width * som_height)}\n",
    "\n",
    "for i in range(num_slices):\n",
    "    # Select the slice\n",
    "    image_slice = image_array[i]\n",
    "    \n",
    "    # Normalize the image slice\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "    \n",
    "    # Reshape image to list of pixels\n",
    "    pixels = image_slice_norm.reshape(-1, input_len)\n",
    "\n",
    "    # Train the SOM\n",
    "    som.random_weights_init(pixels)\n",
    "    #som.train_random(pixels, 1000)\n",
    "    som.train(pixels, 1000)\n",
    "\n",
    "    # Map each pixel to the SOM and assign a label\n",
    "    labels = np.zeros((image_slice.shape[0], image_slice.shape[1]), dtype=int)\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            w = som.winner(image_slice_norm[x, y])\n",
    "            labels[x, y] = w[0] * som_width + w[1]\n",
    "\n",
    "    # Extract pixel values into lists grouped by cluster label\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            cluster_label = labels[x, y]\n",
    "            pixel_value = image_slice[x, y]  # Use original pixel value (before normalization)\n",
    "            pixel_clusters[cluster_label].append(pixel_value)\n",
    "\n",
    "    # Visualize the clusters as images\n",
    "    cluster_images = []\n",
    "    for label in range(som_width * som_height):\n",
    "        # Create an empty image for the cluster\n",
    "        cluster_image = np.zeros_like(image_slice, dtype=np.float32)\n",
    "        # Assign pixel values to the cluster image where labels match\n",
    "        cluster_image[labels == label] = image_slice[labels == label]\n",
    "        cluster_images.append(cluster_image)\n",
    "\n",
    "    # Plot the original and cluster images\n",
    "    fig, axes = plt.subplots(1, som_width * som_height + 1, figsize=(15, 5))\n",
    "    \n",
    "    # Plot the original image\n",
    "    axes[0].imshow(image_slice_norm, cmap='gray')\n",
    "    axes[0].set_title(f'Original Image (Slice {i})')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Plot each cluster image\n",
    "    for j, cluster_image in enumerate(cluster_images):\n",
    "        axes[j + 1].imshow(cluster_image, cmap='gray')\n",
    "        axes[j + 1].set_title(f'Cluster {j}')\n",
    "        axes[j + 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from minisom import MiniSom\n",
    "\n",
    "# Load the NRRD image using SimpleITK\n",
    "nrrd_path = \"C://Users//Administrator//Desktop//GB_ANALYSIS//GB_ANALYSIS_new//20_mm_final//TMH//gldm/105131012/105131012_original_gldm_HighGrayLevelEmphasis.nrrd\"\n",
    "image = sitk.ReadImage(nrrd_path)\n",
    "image_array = sitk.GetArrayFromImage(image)\n",
    "\n",
    "# Check image dimensions\n",
    "if image_array.ndim != 3:\n",
    "    raise ValueError(\"Image array must be a 3D array\")\n",
    "\n",
    "num_slices = image_array.shape[0]\n",
    "\n",
    "def normalize_slice(slice):\n",
    "    max_val = np.max(slice)\n",
    "    if max_val > 0:\n",
    "        return (slice.astype(np.float32) / max_val).clip(0, 1)\n",
    "    else:\n",
    "        return np.zeros_like(slice, dtype=np.float32)  # Return an array of zeros if max is zero\n",
    "\n",
    "# Define 3D SOM parameters\n",
    "som_width = 2\n",
    "som_height = 2\n",
    "som_depth = 1  # New dimension for 3D\n",
    "input_len = image_array.shape[-1] if image_array.ndim == 4 else 1\n",
    "som = MiniSom(x=som_width, y=som_height, input_len=input_len, sigma=1.0, learning_rate=0.5)\n",
    "\n",
    "# Dictionary to hold pixels grouped by cluster labels\n",
    "pixel_clusters = {label: [] for label in range(som_width * som_height * som_depth)}\n",
    "\n",
    "for i in range(num_slices):\n",
    "    # Select the slice\n",
    "    image_slice = image_array[i]\n",
    "    \n",
    "    # Normalize the image slice\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "    \n",
    "    # Reshape image to list of pixels\n",
    "    pixels = image_slice_norm.reshape(-1, input_len)\n",
    "\n",
    "    # Train the SOM\n",
    "    som.random_weights_init(pixels)\n",
    "    som.train(pixels, 1000)\n",
    "\n",
    "    # Map each pixel to the SOM and assign a label\n",
    "    labels = np.zeros((image_slice.shape[0], image_slice.shape[1]), dtype=int)\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            w = som.winner(image_slice_norm[x, y])\n",
    "            labels[x, y] = w[0] * som_height * som_depth + w[1] * som_depth + (i % som_depth)\n",
    "\n",
    "    # Extract pixel values into lists grouped by cluster label\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            cluster_label = labels[x, y]\n",
    "            pixel_value = image_slice[x, y]  # Use original pixel value (before normalization)\n",
    "            pixel_clusters[cluster_label].append(pixel_value)\n",
    "\n",
    "    # Visualize the clusters as images\n",
    "    cluster_images = []\n",
    "    for label in range(som_width * som_height * som_depth):\n",
    "        # Create an empty image for the cluster\n",
    "        cluster_image = np.zeros_like(image_slice, dtype=np.float32)\n",
    "        # Assign pixel values to the cluster image where labels match\n",
    "        cluster_image[labels == label] = image_slice[labels == label]\n",
    "        cluster_images.append(cluster_image)\n",
    "\n",
    "    # Plot the original and cluster images\n",
    "    num_clusters = som_width * som_height * som_depth\n",
    "    fig, axes = plt.subplots(1, num_clusters + 1, figsize=(15, 5))\n",
    "\n",
    "    # Plot the original image\n",
    "    axes[0].imshow(image_slice_norm, cmap='gray')\n",
    "    axes[0].set_title(f'Original Image (Slice {i})')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Plot each cluster image\n",
    "    for j in range(num_clusters):\n",
    "        if j < len(cluster_images):  # Check if cluster_image exists\n",
    "            axes[j + 1].imshow(cluster_images[j], cmap='hot')\n",
    "            axes[j + 1].set_title(f'Cluster {j}')\n",
    "            axes[j + 1].axis('off')\n",
    "        else:\n",
    "            axes[j + 1].axis('off')  # Hide empty axes if no cluster image exists\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from minisom import MiniSom\n",
    "\n",
    "# Load the NRRD image using SimpleITK\n",
    "nrrd_path = \"C://Users//Administrator//Desktop//GB_ANALYSIS//GB_ANALYSIS_new//20_mm_final//TMH//gldm/105131012/105131012_original_gldm_SmallDependenceLowGrayLevelEmphasis.nrrd\"\n",
    "image = sitk.ReadImage(nrrd_path)\n",
    "image_array = sitk.GetArrayFromImage(image)\n",
    "\n",
    "# Check image dimensions\n",
    "if image_array.ndim != 3:\n",
    "    raise ValueError(\"Image array must be a 3D array\")\n",
    "\n",
    "num_slices = image_array.shape[0]\n",
    "\n",
    "def normalize_slice(slice):\n",
    "    max_val = np.max(slice)\n",
    "    if max_val > 0:\n",
    "        return (slice.astype(np.float32) / max_val).clip(0, 1)\n",
    "    else:\n",
    "        return np.zeros_like(slice, dtype=np.float32)\n",
    "\n",
    "# Define SOM parameters\n",
    "som_width = 2\n",
    "som_height = 2\n",
    "som_depth = 1\n",
    "input_len = image_array.shape[-1] if image_array.ndim == 4 else 1\n",
    "som = MiniSom(x=som_width, y=som_height, input_len=input_len, sigma=1.0, learning_rate=0.5)\n",
    "\n",
    "# Prepare a list to hold pixel values from all slices\n",
    "all_pixels = []\n",
    "\n",
    "# Normalize and gather all pixel values from all slices\n",
    "for i in range(num_slices):\n",
    "    image_slice = image_array[i]\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "    pixels = image_slice_norm.reshape(-1, input_len)\n",
    "    all_pixels.append(pixels)\n",
    "\n",
    "# Concatenate all pixel values into a single array\n",
    "all_pixels = np.vstack(all_pixels)\n",
    "\n",
    "# Train the SOM on the combined pixel values\n",
    "som.random_weights_init(all_pixels)\n",
    "som.train(all_pixels, 1000)\n",
    "\n",
    "# Initialize a dictionary to hold pixel values grouped by cluster labels\n",
    "pixel_clusters = {label: [] for label in range(som_width * som_height * som_depth)}\n",
    "\n",
    "# Now map each pixel in each slice to the trained SOM\n",
    "for i in range(num_slices):\n",
    "    image_slice = image_array[i]\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "    labels = np.zeros((image_slice.shape[0], image_slice.shape[1]), dtype=int)\n",
    "\n",
    "    # Map each pixel to the SOM\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            w = som.winner(image_slice_norm[x, y])\n",
    "            labels[x, y] = w[0] * som_height * som_depth + w[1] * som_depth + (i % som_depth)\n",
    "\n",
    "    # Extract pixel values into lists grouped by cluster label\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            cluster_label = labels[x, y]\n",
    "            pixel_value = image_slice[x, y]\n",
    "            pixel_clusters[cluster_label].append(pixel_value)\n",
    "\n",
    "    # Visualize the clusters as images\n",
    "    cluster_images = []\n",
    "    for label in range(som_width * som_height * som_depth):\n",
    "        cluster_image = np.zeros_like(image_slice, dtype=np.float32)\n",
    "        cluster_image[labels == label] = image_slice[labels == label]\n",
    "        cluster_images.append(cluster_image)\n",
    "\n",
    "    # Plot the original and cluster images\n",
    "    num_clusters = som_width * som_height * som_depth\n",
    "    fig, axes = plt.subplots(1, num_clusters + 1, figsize=(15, 5))\n",
    "\n",
    "    # Plot the original image\n",
    "    axes[0].imshow(image_slice_norm, cmap='gray')\n",
    "    axes[0].set_title(f'Original Image (Slice {i})')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Plot each cluster image\n",
    "    for j in range(num_clusters):\n",
    "        if j < len(cluster_images):  # Check if cluster_image exists\n",
    "            axes[j + 1].imshow(cluster_images[j], cmap='gray')\n",
    "            axes[j + 1].set_title(f'Cluster {j}')\n",
    "            axes[j + 1].axis('off')\n",
    "        else:\n",
    "            axes[j + 1].axis('off')  # Hide empty axes if no cluster image exists\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GB Yellamma finally working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from minisom import MiniSom\n",
    "\n",
    "# Load the NRRD image using SimpleITK\n",
    "nrrd_path = \"C://Users//Administrator//Desktop//GB_ANALYSIS//GB_ANALYSIS_new//20_mm_final//TMH//gldm/108689672/108689672_original_gldm_GrayLevelVariance.nrrd\"\n",
    "\n",
    "import re \n",
    "r1=re.split(string=nrrd_path, pattern='\\/')\n",
    "r2=re.split(string=r1[-1], pattern='_')\n",
    "fname=r2[-1]\n",
    "\n",
    "image = sitk.ReadImage(nrrd_path)\n",
    "image_array = sitk.GetArrayFromImage(image)\n",
    "\n",
    "# Check image dimensions\n",
    "if image_array.ndim != 3:\n",
    "    raise ValueError(\"Image array must be a 3D array\")\n",
    "\n",
    "num_slices = image_array.shape[0]\n",
    "\n",
    "def normalize_slice(slice):\n",
    "    max_val = np.max(slice)\n",
    "    if max_val > 0:\n",
    "        return (slice.astype(np.float32) / max_val).clip(0, 1)\n",
    "    else:\n",
    "        return np.zeros_like(slice, dtype=np.float32)\n",
    "\n",
    "# Define larger SOM parameters\n",
    "som_width = 2\n",
    "som_height= 2\n",
    "som_depth = 1  # Increase depth for 3D SOM\n",
    "input_len = image_array.shape[-1] if image_array.ndim == 4 else 1\n",
    "som = MiniSom(x=som_width, y=som_height, input_len=input_len, sigma=1.0, learning_rate=0.5)\n",
    "\n",
    "# Prepare a list to hold pixel values from all slices\n",
    "all_pixels = []\n",
    "\n",
    "# Normalize and gather all pixel values from all slices\n",
    "for i in range(num_slices):\n",
    "    image_slice = image_array[i]\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "    pixels = image_slice_norm.reshape(-1, input_len)\n",
    "    all_pixels.append(pixels)\n",
    "\n",
    "# Concatenate all pixel values into a single array\n",
    "all_pixels = np.vstack(all_pixels)\n",
    "\n",
    "# Train the SOM on the combined pixel values\n",
    "som.random_weights_init(all_pixels)\n",
    "som.train(all_pixels, 10000)  # Increased training iterations\n",
    "\n",
    "# Initialize a dictionary to hold pixel values grouped by cluster labels\n",
    "pixel_clusters = {label: [] for label in range(som_width * som_height * som_depth)}\n",
    "\n",
    "# Map each pixel in each slice to the trained SOM\n",
    "#for i in range(1, num_slices - 1):\n",
    "for i in range(1, num_slices -1):\n",
    "    image_slice = image_array[i]\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "    labels = np.zeros((image_slice.shape[0], image_slice.shape[1]), dtype=int)\n",
    "\n",
    "    # Map each pixel to the SOM\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            w = som.winner(image_slice_norm[x, y])\n",
    "            labels[x, y] = w[0] * som_height * som_depth + w[1] * som_depth + (i % som_depth)\n",
    "\n",
    "    # Extract pixel values into lists grouped by cluster label\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            cluster_label = labels[x, y]\n",
    "            pixel_value = image_slice[x, y]\n",
    "            pixel_clusters[cluster_label].append(pixel_value)\n",
    "\n",
    "    # Visualize the clusters as images\n",
    "    cluster_images = []\n",
    "    for label in range(som_width * som_height * som_depth):\n",
    "        cluster_image = np.zeros_like(image_slice, dtype=np.float32)\n",
    "        cluster_image[labels == label] = image_slice[labels == label]\n",
    "        cluster_images.append(cluster_image)\n",
    "\n",
    "    # Plot the original and cluster images\n",
    "    num_clusters = som_width * som_height * som_depth\n",
    "    fig, axes = plt.subplots(1, num_clusters + 1, figsize=(15, 5))\n",
    "\n",
    "    # Plot the original image\n",
    "    axes[0].imshow(image_slice_norm, cmap='gray')\n",
    "    #axes[0].set_title(fname,f'(Slice {i})')\n",
    "    axes[0].set_title(f'{fname} (Slice {i})')\n",
    "\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Plot each cluster image\n",
    "    for j in range(num_clusters):\n",
    "        if j < len(cluster_images):  # Check if cluster_image exists\n",
    "            im=axes[j + 1].imshow(cluster_images[j], cmap='hot')\n",
    "            axes[j + 1].set_title(f'Cluster {j}')\n",
    "            axes[j + 1].axis('off')\n",
    "        \n",
    "            # Add a colorbar for the current cluster image\n",
    "            plt.colorbar(im, ax=axes[j + 1])\n",
    "        \n",
    "        else:\n",
    "            axes[j + 1].axis('off')  # Hide empty axes if no cluster image exists\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on 5 Patients\n",
    "base_path = 'C:/Users/Administrator/Desktop/GB_ANALYSIS/GB_ANALYSIS_new/20_mm_final/gb_test_5/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "base_path = 'C:/Users/Administrator/Desktop/GB_ANALYSIS/GB_ANALYSIS_new/20_mm_final/gb_test_5/'\n",
    "\n",
    "def get_first_nrrd_in_folders(base_path):\n",
    "    # Loop through each folder in the base path\n",
    "    for folder in os.listdir(base_path):\n",
    "        folder_path = os.path.join(base_path, folder)\n",
    "        \n",
    "        # Check if it is a directory\n",
    "        if os.path.isdir(folder_path):\n",
    "            # Use glob to find .nrrd files in the current folder\n",
    "            nrrd_files = glob.glob(os.path.join(folder_path, \"*.nrrd\"))\n",
    "            \n",
    "            # If we have any .nrrd files, return the first one\n",
    "            if nrrd_files:\n",
    "                print(f\"First .nrrd file in folder {folder}: {nrrd_files[0]}\")\n",
    "            else:\n",
    "                print(f\"No .nrrd files found in folder {folder}\")\n",
    "\n",
    "# Usage example\n",
    "#base_path = '/path/to/your/directory'\n",
    "get_first_nrrd_in_folders(base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get volumes from each clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(main_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from minisom import MiniSom\n",
    "import re \n",
    "import tqdm\n",
    "\n",
    "gb_main=[]\n",
    "\n",
    "gb_pat=[]\n",
    "ac1=[]\n",
    "ac2=[]\n",
    "ac3=[]\n",
    "ac4=[]\n",
    "\n",
    "# Load the NRRD image using SimpleITK\n",
    "#nrrd_path = \"C://Users//Administrator//Desktop//GB_ANALYSIS//GB_ANALYSIS_new//20_mm_final//TMH//gldm/108689672/108689672_original_gldm_GrayLevelVariance.nrrd\"\n",
    "#base_path=\"C:/Users/Administrator/Desktop/GB_ANALYSIS/GB_ANALYSIS_new/20_mm_final/gb_test_5/\"\n",
    "base_path='C://Users//Administrator//Desktop//GB_ANALYSIS//GB_ANALYSIS_new//20_mm_final//TMH//gldm/'\n",
    "# Loop through each folder in the base path\n",
    "for folder in  tqdm.tqdm(os.listdir(base_path)):\n",
    "    folder_path = os.path.join(base_path, folder)\n",
    "    \n",
    "    # Check if it is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Use glob to find .nrrd files in the current folder\n",
    "        nrrd_files = glob.glob(os.path.join(folder_path, \"*.nrrd\"))\n",
    "\n",
    "        # If we have any .nrrd files, return the first one\n",
    "        if nrrd_files:\n",
    "            #print(f\"First .nrrd file in folder {folder}: {nrrd_files[0]}\")\n",
    "\n",
    "            r1=re.split(string=nrrd_files[6], pattern='\\/')\n",
    "            #print(r1[-1])\n",
    "            r2=re.split(string=r1[-1], pattern='\\\\\\\\')\n",
    "            #r2=re.split(string=r1[-1], pattern='_')\n",
    "            #print(r2[-1])\n",
    "            fname=r2[-1]\n",
    "            r3=re.split(string=fname, pattern='_')[0]\n",
    "            \n",
    "            \n",
    "            print(fname)\n",
    "            test.append(fname)\n",
    "            gb_pat.append(r3)\n",
    "            \n",
    "            print('reading image')\n",
    "            image = sitk.ReadImage(nrrd_files[6])\n",
    "            image_array = sitk.GetArrayFromImage(image)\n",
    "            print('read image')\n",
    "\n",
    "            # Check image dimensions\n",
    "            if image_array.ndim != 3:\n",
    "                raise ValueError(\"Image array must be a 3D array\")\n",
    "            num_slices = image_array.shape[0]\n",
    "            print('number of slices = ',num_slices)\n",
    "\n",
    "            \n",
    "            def normalize_slice(slice):\n",
    "                max_val = np.max(slice)\n",
    "                if max_val > 0:\n",
    "                    return (slice.astype(np.float32) / max_val).clip(0, 1)\n",
    "                else:\n",
    "                    return np.zeros_like(slice, dtype=np.float32)\n",
    "\n",
    "                \n",
    "            # Define larger SOM parameters\n",
    "            som_width = 2\n",
    "            som_height= 2\n",
    "            som_depth = 1  # Increase depth for 3D SOM\n",
    "            input_len = image_array.shape[-1] if image_array.ndim == 4 else 1\n",
    "            som = MiniSom(x=som_width, y=som_height, input_len=input_len, sigma=1.0, learning_rate=0.5)\n",
    "\n",
    "            # Prepare a list to hold pixel values from all slices\n",
    "            all_pixels = []\n",
    "\n",
    "            # Normalize and gather all pixel values from all slices\n",
    "            for i in range(num_slices):\n",
    "                image_slice = image_array[i]\n",
    "                image_slice_norm = normalize_slice(image_slice)\n",
    "                pixels = image_slice_norm.reshape(-1, input_len)\n",
    "                all_pixels.append(pixels)\n",
    "\n",
    "            # Concatenate all pixel values into a single array\n",
    "            all_pixels = np.vstack(all_pixels)\n",
    "\n",
    "            # Train the SOM on the combined pixel values\n",
    "            som.random_weights_init(all_pixels)\n",
    "            print('training som')\n",
    "            som.train(all_pixels, 30000)  # Increased training iterations\n",
    "\n",
    "            # Initialize a dictionary to hold pixel values grouped by cluster labels\n",
    "            pixel_clusters = {label: [] for label in range(som_width * som_height * som_depth)}\n",
    "\n",
    "            # four clusters\n",
    "\n",
    "            c1=[]\n",
    "            c2=[]\n",
    "            c3=[]\n",
    "            c4=[]\n",
    "\n",
    "            # Map each pixel in each slice to the trained SOM\n",
    "            #for i in range(1, num_slices - 1):\n",
    "            print('mapping voxels to trained SOM')\n",
    "            #for i in range(1, num_slices -1):\n",
    "            for i in range (num_slices):\n",
    "                image_slice = image_array[i]\n",
    "                image_slice_norm = normalize_slice(image_slice)\n",
    "                labels = np.zeros((image_slice.shape[0], image_slice.shape[1]), dtype=int)\n",
    "\n",
    "                # Map each pixel to the SOM\n",
    "                for x in range(image_slice.shape[0]):\n",
    "                    for y in range(image_slice.shape[1]):\n",
    "                        w = som.winner(image_slice_norm[x, y])\n",
    "                        labels[x, y] = w[0] * som_height * som_depth + w[1] * som_depth + (i % som_depth)\n",
    "\n",
    "                # Extract pixel values into lists grouped by cluster label\n",
    "                for x in range(image_slice.shape[0]):\n",
    "                    for y in range(image_slice.shape[1]):\n",
    "                        cluster_label = labels[x, y]\n",
    "                        pixel_value = image_slice[x, y]\n",
    "                        pixel_clusters[cluster_label].append(pixel_value)\n",
    "\n",
    "                # Visualize the clusters as images\n",
    "                cluster_images = []\n",
    "                for label in range(som_width * som_height * som_depth):\n",
    "                    cluster_image = np.zeros_like(image_slice, dtype=np.float32)\n",
    "                    cluster_image[labels == label] = image_slice[labels == label]\n",
    "                    cluster_images.append(cluster_image)\n",
    "                #print('creating 3d clusters')\n",
    "                # join all the 2d clusters into 1 3d volume.\n",
    "                c1.append(cluster_images[0]) # first cluster\n",
    "                c2.append(cluster_images[1]) # second cluster\n",
    "                c3.append(cluster_images[2]) # third cluster\n",
    "                c4.append(cluster_images[3]) # fourth cluster\n",
    "\n",
    "                # Plot the original and cluster images\n",
    "                num_clusters = som_width * som_height * som_depth\n",
    "                fig, axes = plt.subplots(1, num_clusters + 1, figsize=(15, 5))\n",
    "\n",
    "                # Plot the original image\n",
    "                axes[0].imshow(image_slice_norm, cmap='gray')\n",
    "                #axes[0].set_title(fname,f'(Slice {i})')\n",
    "                axes[0].set_title(f'{fname} (Slice {i})')\n",
    "\n",
    "                axes[0].axis('off')\n",
    "\n",
    "                # Plot each cluster image\n",
    "                for j in range(num_clusters):\n",
    "                    if j < len(cluster_images):  # Check if cluster_image exists\n",
    "                        im=axes[j + 1].imshow(cluster_images[j], cmap='hot')\n",
    "                        axes[j + 1].set_title(f'Cluster {j+1}')\n",
    "                        axes[j + 1].axis('off')\n",
    "\n",
    "                        # Add a colorbar for the current cluster image\n",
    "                        plt.colorbar(im, ax=axes[j + 1])\n",
    "\n",
    "                    else:\n",
    "                        axes[j + 1].axis('off')  # Hide empty axes if no cluster image exists\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            #print(c1)\n",
    "            # applying PCA for each cluster \n",
    "            ac1.append(gb_pca(c1))\n",
    "            ac2.append(gb_pca(c2))\n",
    "            ac3.append(gb_pca(c3))\n",
    "            ac4.append(gb_pca(c4))\n",
    "            \n",
    "main_cluster=[gb_pat,ac1,ac2,ac3,ac4]                         \n",
    "\n",
    "final_pca_df=pd.DataFrame(main_cluster,index=['pat_id','cluster1','cluster2','cluster3','cluster4']).T\n",
    "\n",
    "final_pca_df.to_csv(fname+'_voxrad_pca_on_4_SOM_clusters.csv')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(string=fname, pattern='_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(main_cluster,index=['cluster1','cluster2','cluster3','cluster4']).T.to_csv('4clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(main_cluster,index=['cluster1','cluster2','cluster3','cluster4']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(c4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(np.array(c1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "77*59*63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "77*3717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_pca(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_pca(c5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        def gb_pca(s):\n",
    "            nc1=np.array(s)\n",
    "            print('cluster  shape:\\t',np.shape(nc1))\n",
    "            #plt.imshow(n2.reshape(n2.shape[0],-1))\n",
    "            ncf1=nc1.flatten()\n",
    "            pca = PCA(n_components=1)\n",
    "            X_pca = pca.fit_transform(ncf1.reshape(-1, 1))\n",
    "\n",
    "            print('flattened shape:',X_pca.shape)\n",
    "            X_pca[0]\n",
    "            print('pc1 value:     ',float(X_pca[0]))\n",
    "            return float(X_pca[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2=np.array(c2)\n",
    "#plt.imshow(n2.reshape(n2.shape[0],-1))\n",
    "n3=n2.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Apply PCA and reduce to 1 component\n",
    "pca = PCA(n_components=1)\n",
    "X_pca = pca.fit_transform(n3.reshape(-1, 1))\n",
    "\n",
    "print(X_pca.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(X_pca[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattened PCA\n",
    "#PCA on flattened data\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from lifelines import CoxPHFitter\n",
    "import pandas as pd\n",
    "\n",
    "# Example data: \n",
    "# Lets assume you have a list of volumetric data, where each volumetric feature is a 3D array\n",
    "# data_vol1 (100 observations, X1 x Y1 x Z1), data_vol2 (100 observations, X2 x Y2 x Z2)\n",
    "\n",
    "# Function to flatten the volumetric data (reshape 3D to 1D)\n",
    "def flatten_volumetric_data(vol_data):\n",
    "    return vol_data.reshape(vol_data.shape[0], -1)  # Flatten the 3D data to 2D (n_samples, n_features)\n",
    "\n",
    "# Example data (Random generation for illustration)\n",
    "# Let's assume the data has a shape (100, 10, 10, 10) for both volumetric features\n",
    "data_vol1 = np.random.rand(167, 10, 10, 10)\n",
    "data_vol2 = np.random.rand(167, 10, 10, 10)\n",
    "\n",
    "# Flatten both volumetric data\n",
    "data_vol1_flat = flatten_volumetric_data(data_vol1)\n",
    "data_vol2_flat = flatten_volumetric_data(data_vol2)\n",
    "\n",
    "# Apply PCA to each flattened volumetric feature\n",
    "def apply_pca_and_get_first_component(data):\n",
    "    pca = PCA(n_components=1)  # Reduce to 1 component (first principal component)\n",
    "    pca_result = pca.fit_transform(data)\n",
    "    print('pca_result',pca_result)\n",
    "    print(len(pca_result))\n",
    "    return pca_result\n",
    "\n",
    "# Get PCA results (single component) for both features\n",
    "pca_vol1 = apply_pca_and_get_first_component(data_vol1_flat)\n",
    "pca_vol2 = apply_pca_and_get_first_component(data_vol2_flat)\n",
    "\n",
    "# Create a DataFrame for Cox model input, assuming you have survival data (e.g., duration and event)\n",
    "# Example: duration and event columns (randomly generated for illustration)\n",
    "duration = np.random.rand(167) * 10  # Survival duration\n",
    "event = np.random.randint(0, 2, 167)  # Event occurred (1) or censored (0)\n",
    "\n",
    "# Combine the PCA results into a DataFrame (columns: PCA of vol1, PCA of vol2)\n",
    "X_pca = np.hstack([pca_vol1, pca_vol2])\n",
    "df = pd.DataFrame(X_pca, columns=['PCA_vol1', 'PCA_vol2'])\n",
    "df['duration'] = duration\n",
    "df['event'] = event\n",
    "\n",
    "# Fit the Cox Proportional Hazards model using the lifelines package\n",
    "cox = CoxPHFitter()\n",
    "cox.fit(df, duration_col='duration', event_col='event')\n",
    "\n",
    "# Print the Cox model summary\n",
    "cox.print_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vol1_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = sitk.ReadImage(nrrd_path)\n",
    "image_array = sitk.GetArrayFromImage(image)\n",
    "\n",
    "# Check image dimensions\n",
    "if image_array.ndim != 3:\n",
    "    raise ValueError(\"Image array must be a 3D array\")\n",
    "\n",
    "num_slices = image_array.shape[0]\n",
    "\n",
    "def normalize_slice(slice):\n",
    "    max_val = np.max(slice)\n",
    "    if max_val > 0:\n",
    "        return (slice.astype(np.float32) / max_val).clip(0, 1)\n",
    "    else:\n",
    "        return np.zeros_like(slice, dtype=np.float32)\n",
    "\n",
    "# Define larger SOM parameters\n",
    "som_width = 2\n",
    "som_height= 2\n",
    "som_depth = 1  # Increase depth for 3D SOM\n",
    "input_len = image_array.shape[-1] if image_array.ndim == 4 else 1\n",
    "som = MiniSom(x=som_width, y=som_height, input_len=input_len, sigma=1.0, learning_rate=0.5)\n",
    "\n",
    "# Prepare a list to hold pixel values from all slices\n",
    "all_pixels = []\n",
    "\n",
    "# Normalize and gather all pixel values from all slices\n",
    "for i in range(num_slices):\n",
    "    image_slice = image_array[i]\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "    pixels = image_slice_norm.reshape(-1, input_len)\n",
    "    all_pixels.append(pixels)\n",
    "\n",
    "# Concatenate all pixel values into a single array\n",
    "all_pixels = np.vstack(all_pixels)\n",
    "\n",
    "# Train the SOM on the combined pixel values\n",
    "som.random_weights_init(all_pixels)\n",
    "som.train(all_pixels, 10000)  # Increased training iterations\n",
    "\n",
    "# Initialize a dictionary to hold pixel values grouped by cluster labels\n",
    "pixel_clusters = {label: [] for label in range(som_width * som_height * som_depth)}\n",
    "\n",
    "\n",
    "# four clusters\n",
    "\n",
    "c1=[]\n",
    "c2=[]\n",
    "c3=[]\n",
    "c4=[]\n",
    "\n",
    "# Map each pixel in each slice to the trained SOM\n",
    "#for i in range(1, num_slices - 1):\n",
    "for i in range(1, num_slices -1):\n",
    "    image_slice = image_array[i]\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "    labels = np.zeros((image_slice.shape[0], image_slice.shape[1]), dtype=int)\n",
    "\n",
    "    # Map each pixel to the SOM\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            w = som.winner(image_slice_norm[x, y])\n",
    "            labels[x, y] = w[0] * som_height * som_depth + w[1] * som_depth + (i % som_depth)\n",
    "\n",
    "    # Extract pixel values into lists grouped by cluster label\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            cluster_label = labels[x, y]\n",
    "            pixel_value = image_slice[x, y]\n",
    "            pixel_clusters[cluster_label].append(pixel_value)\n",
    "\n",
    "    # Visualize the clusters as images\n",
    "    cluster_images = []\n",
    "    for label in range(som_width * som_height * som_depth):\n",
    "        cluster_image = np.zeros_like(image_slice, dtype=np.float32)\n",
    "        cluster_image[labels == label] = image_slice[labels == label]\n",
    "        cluster_images.append(cluster_image)\n",
    "    \n",
    "    # join all the 2d clusters into 1 3d volume.\n",
    "    c1.append(cluster_images[0]) # first cluster\n",
    "    c2.append(cluster_images[1]) # second cluster\n",
    "    c3.append(cluster_images[2]) # third cluster\n",
    "    c4.append(cluster_images[3]) # fourth cluster\n",
    "\n",
    "    # Plot the original and cluster images\n",
    "    num_clusters = som_width * som_height * som_depth\n",
    "    fig, axes = plt.subplots(1, num_clusters + 1, figsize=(15, 5))\n",
    "\n",
    "    # Plot the original image\n",
    "    axes[0].imshow(image_slice_norm, cmap='gray')\n",
    "    #axes[0].set_title(fname,f'(Slice {i})')\n",
    "    axes[0].set_title(f'{fname} (Slice {i})')\n",
    "\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Plot each cluster image\n",
    "    for j in range(num_clusters):\n",
    "        if j < len(cluster_images):  # Check if cluster_image exists\n",
    "            im=axes[j + 1].imshow(cluster_images[j], cmap='hot')\n",
    "            axes[j + 1].set_title(f'Cluster {j}')\n",
    "            axes[j + 1].axis('off')\n",
    "        \n",
    "            # Add a colorbar for the current cluster image\n",
    "            plt.colorbar(im, ax=axes[j + 1])\n",
    "        \n",
    "        else:\n",
    "            axes[j + 1].axis('off')  # Hide empty axes if no cluster image exists\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(c1[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "47*50*44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pixels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from minisom import MiniSom\n",
    "\n",
    "# Load the NRRD image using SimpleITK\n",
    "nrrd_path = \"C://Users//Administrator//Desktop//GB_ANALYSIS//GB_ANALYSIS_new//20_mm_final//TMH//gldm/108689672/108689672_original_gldm_GrayLevelVariance.nrrd\"\n",
    "\n",
    "import re \n",
    "r1 = re.split(string=nrrd_path, pattern='\\/')\n",
    "r2 = re.split(string=r1[-1], pattern='_')\n",
    "fname = r2[-1]\n",
    "\n",
    "image = sitk.ReadImage(nrrd_path)\n",
    "image_array = sitk.GetArrayFromImage(image)\n",
    "\n",
    "# Check image dimensions\n",
    "if image_array.ndim != 3:\n",
    "    raise ValueError(\"Image array must be a 3D array\")\n",
    "\n",
    "num_slices = image_array.shape[0]\n",
    "\n",
    "def normalize_slice(slice):\n",
    "    max_val = np.max(slice)\n",
    "    if max_val > 0:\n",
    "        return (slice.astype(np.float32) / max_val).clip(0, 1)\n",
    "    else:\n",
    "        return np.zeros_like(slice, dtype=np.float32)\n",
    "\n",
    "# Define SOM parameters (2x2 grid with 4 clusters)\n",
    "som_width = 2\n",
    "som_height = 2\n",
    "som_depth = 1  # This is a 2D SOM for simplicity, can extend to 3D if needed\n",
    "input_len = image_array.shape[-1] if image_array.ndim == 4 else 1\n",
    "som = MiniSom(x=som_width, y=som_height, input_len=input_len, sigma=1.0, learning_rate=0.5)\n",
    "\n",
    "# Prepare a list to hold pixel values from all slices\n",
    "all_pixels = []\n",
    "\n",
    "# Normalize and gather all pixel values from all slices\n",
    "for i in range(num_slices):\n",
    "    image_slice = image_array[i]\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "    pixels = image_slice_norm.reshape(-1, input_len)\n",
    "    all_pixels.append(pixels)\n",
    "\n",
    "# Concatenate all pixel values into a single array\n",
    "all_pixels = np.vstack(all_pixels)\n",
    "\n",
    "# Train the SOM on the combined pixel values\n",
    "som.random_weights_init(all_pixels)\n",
    "som.train(all_pixels, 10000)  # Increased training iterations\n",
    "\n",
    "# Initialize a dictionary to hold 3D volumes for each cluster (4 clusters total)\n",
    "cluster_3d_volumes = {label: np.zeros_like(image_array[0], dtype=np.float32) for label in range(som_width * som_height)}\n",
    "\n",
    "# Map each pixel in each slice to the trained SOM and fill in the 3D cluster volumes\n",
    "for i in range(num_slices):\n",
    "    image_slice = image_array[i]\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "    labels = np.zeros((image_slice.shape[0], image_slice.shape[1]), dtype=int)\n",
    "\n",
    "    # Map each pixel to the SOM\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            w = som.winner(image_slice_norm[x, y])\n",
    "            labels[x, y] = w[0] * som_height + w[1]  # Simple label assignment for a 2x2 SOM grid\n",
    "\n",
    "    # Assign pixel values to the corresponding cluster's 3D volume\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            cluster_label = labels[x, y]\n",
    "            pixel_value = image_slice[x, y]\n",
    "            cluster_3d_volumes[cluster_label][i, x, y] = pixel_value\n",
    "\n",
    "# Now, visualize the 3D volumes for each cluster\n",
    "num_clusters = som_width * som_height\n",
    "for cluster_label in range(num_clusters):\n",
    "    # Plot the 3D volume for the current cluster (slice by slice)\n",
    "    fig, axes = plt.subplots(1, num_slices, figsize=(15, 5))\n",
    "\n",
    "    # Loop through each slice for the current cluster\n",
    "    for i in range(num_slices):\n",
    "        axes[i].imshow(cluster_3d_volumes[cluster_label][i], cmap='hot')\n",
    "        axes[i].set_title(f\"Cluster {cluster_label} - Slice {i}\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_3d_images[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(cluster_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(cluster_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(cluster_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cluster_images[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### Apply PCA for Each Cluster and Extract the First Principal Component (PC1)\n",
    "#### Well now apply PCA separately for each cluster and extract the first principal component (PC1), which will serve as the ### summary feature for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize an empty list to store the first principal component for each cluster\n",
    "pca_features = []\n",
    "\n",
    "# Standardize and apply PCA for each cluster\n",
    "for cluster_idx in range(num_clusters):\n",
    "    # Get the voxel data for the current cluster\n",
    "    cluster_data = df.iloc[:, cluster_idx * voxels_per_cluster:(cluster_idx + 1) * voxels_per_cluster]\n",
    "    \n",
    "    # Standardize the data (important for PCA)\n",
    "    scaler = StandardScaler()\n",
    "    cluster_data_scaled = scaler.fit_transform(cluster_data)\n",
    "    \n",
    "    # Apply PCA and get the first principal component (PC1)\n",
    "    pca = PCA(n_components=1)\n",
    "    pca_result = pca.fit_transform(cluster_data_scaled)\n",
    "    \n",
    "    # Append the first principal component for this cluster to the list\n",
    "    pca_features.append(pca_result)\n",
    "\n",
    "# Convert the list of PCA features into a DataFrame\n",
    "pca_features = np.hstack(pca_features)  # Concatenate the PC1 scores for all clusters\n",
    "df_pca = pd.DataFrame(pca_features, columns=[f'PC_Cluster_{i+1}' for i in range(num_clusters)])\n",
    "\n",
    "# Add survival data (time and event) back to the DataFrame\n",
    "df_pca['time'] = df['time']\n",
    "df_pca['event'] = df['event']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,c in enumerate(cluster_images):\n",
    "    print(x,\"\\n\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Initialize an empty list to store the first principal component for each cluster\n",
    "pca_features = []\n",
    "\n",
    "# Standardize and apply PCA for each cluster\n",
    "for c in cluster_images:\n",
    "    # Get the voxel data for the current cluster\n",
    "    #cluster_data = df.iloc[:, cluster_idx * voxels_per_cluster:(cluster_idx + 1) * voxels_per_cluster]\n",
    "    cluster_data=c\n",
    "    print('raw data', cluster_data)\n",
    "    plt.imshow(cluster_data)\n",
    "    \n",
    "    # Standardize the data (important for PCA)\n",
    "    scaler = StandardScaler()\n",
    "    cluster_data_scaled = scaler.fit_transform(cluster_data)\n",
    "    print('scaled data', cluster_data_scaled)\n",
    "    plt.imshow(cluster_data_scaled)\n",
    "    \n",
    "    # Apply PCA and get the first principal component (PC1)\n",
    "    pca = PCA(n_components=1)\n",
    "    pca_result = pca.fit_transform(cluster_data_scaled)\n",
    "    \n",
    "    # Append the first principal component for this cluster to the list\n",
    "    pca_features.append(pca_result)\n",
    "\n",
    "# Convert the list of PCA features into a DataFrame\n",
    "pca_features = np.hstack(pca_features)  # Concatenate the PC1 scores for all clusters\n",
    "df_pca = pd.DataFrame(pca_features, columns=[f'PC_Cluster_{i+1}' for i in range(num_clusters)])\n",
    "\n",
    "# # Add survival data (time and event) back to the DataFrame\n",
    "# df_pca['time'] = df['time']\n",
    "# df_pca['event'] = df['event']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize an empty list to store the first principal component for each cluster\n",
    "pca_features = []\n",
    "\n",
    "# Iterate through the clusters\n",
    "for i, c in enumerate(cluster_images):\n",
    "    # Get the voxel data for the current cluster (c is assumed to be a 2D array)\n",
    "    cluster_data = c\n",
    "    print(f'Raw data for cluster {i}:', cluster_data)\n",
    "    \n",
    "    # Plot raw data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f'Raw Data - Cluster {i}')\n",
    "    plt.imshow(cluster_data, cmap='hot')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Standardize the data (important for PCA)\n",
    "    scaler = StandardScaler()\n",
    "    cluster_data_scaled = scaler.fit_transform(cluster_data)\n",
    "    print(f'Scaled data for cluster {i}:', cluster_data_scaled)\n",
    "    \n",
    "    # Plot scaled data\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(f'Scaled Data - Cluster {i}')\n",
    "    plt.imshow(cluster_data_scaled, cmap='hot')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.show()  # Show the plots for raw and scaled data\n",
    "\n",
    "    # Apply PCA and get the first principal component (PC1)\n",
    "    pca = PCA(n_components=1)\n",
    "    pca_result = pca.fit_transform(cluster_data_scaled)\n",
    "    \n",
    "    # Append the first principal component for this cluster to the list\n",
    "    pca_features.append(pca_result)\n",
    "\n",
    "    # Optionally, you can visualize the first principal component if needed\n",
    "    plt.figure()\n",
    "    plt.title(f'First Principal Component - Cluster {i}')\n",
    "    plt.plot(pca_result)\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Principal Component Value')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(pca_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(pca_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on flattend data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA on flattened data\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from lifelines import CoxPHFitter\n",
    "import pandas as pd\n",
    "\n",
    "# Example data: \n",
    "# Lets assume you have a list of volumetric data, where each volumetric feature is a 3D array\n",
    "# data_vol1 (100 observations, X1 x Y1 x Z1), data_vol2 (100 observations, X2 x Y2 x Z2)\n",
    "\n",
    "# Function to flatten the volumetric data (reshape 3D to 1D)\n",
    "def flatten_volumetric_data(vol_data):\n",
    "    return vol_data.reshape(vol_data.shape[0], -1)  # Flatten the 3D data to 2D (n_samples, n_features)\n",
    "\n",
    "# Example data (Random generation for illustration)\n",
    "# Let's assume the data has a shape (100, 10, 10, 10) for both volumetric features\n",
    "data_vol1 = np.random.rand(100, 10, 10, 10)\n",
    "data_vol2 = np.random.rand(100, 10, 10, 10)\n",
    "\n",
    "# Flatten both volumetric data\n",
    "data_vol1_flat = flatten_volumetric_data(data_vol1)\n",
    "data_vol2_flat = flatten_volumetric_data(data_vol2)\n",
    "\n",
    "# Apply PCA to each flattened volumetric feature\n",
    "def apply_pca_and_get_first_component(data):\n",
    "    pca = PCA(n_components=1)  # Reduce to 1 component (first principal component)\n",
    "    pca_result = pca.fit_transform(data)\n",
    "    return pca_result\n",
    "\n",
    "# Get PCA results (single component) for both features\n",
    "pca_vol1 = apply_pca_and_get_first_component(data_vol1_flat)\n",
    "pca_vol2 = apply_pca_and_get_first_component(data_vol2_flat)\n",
    "\n",
    "# Create a DataFrame for Cox model input, assuming you have survival data (e.g., duration and event)\n",
    "# Example: duration and event columns (randomly generated for illustration)\n",
    "duration = np.random.rand(100) * 10  # Survival duration\n",
    "event = np.random.randint(0, 2, 100)  # Event occurred (1) or censored (0)\n",
    "\n",
    "# Combine the PCA results into a DataFrame (columns: PCA of vol1, PCA of vol2)\n",
    "X_pca = np.hstack([pca_vol1, pca_vol2])\n",
    "df = pd.DataFrame(X_pca, columns=['PCA_vol1', 'PCA_vol2'])\n",
    "df['duration'] = duration\n",
    "df['event'] = event\n",
    "\n",
    "# Fit the Cox Proportional Hazards model using the lifelines package\n",
    "cox = CoxPHFitter()\n",
    "cox.fit(df, duration_col='duration', event_col='event')\n",
    "\n",
    "# Print the Cox model summary\n",
    "cox.print_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Coefficients and Hazard Ratios:\n",
    "# For PCA_vol1:\n",
    "\n",
    "# coef: -0.13  This is the estimated coefficient for PCA_vol1 in the Cox model. \n",
    "A negative coefficient suggests that as the value of PCA_vol1 increases, the hazard (risk of the event happening) decreases.\n",
    "# In other words, higher values of PCA_vol1 are associated with a lower risk of the event.\n",
    "\n",
    "# exp(coef): 0.88  This is the hazard ratio (HR) corresponding to PCA_vol1.\n",
    "# A hazard ratio of 0.88 means that for each unit increase in PCA_vol1,the hazard of the event decreases by 12%(since 1 0.88=\n",
    "# 0.12\n",
    "# 10.88=0.12).\n",
    "# se(coef): 0.12  The standard error of the coefficient. This measures the variability or uncertainty in the estimated coefficient.\n",
    "# 95% Confidence Interval: The 95% confidence interval for the coefficient is from -0.37 to 0.10. Since zero is included in this interval, we cannot conclusively say that the effect of PCA_vol1 is statistically significant.\n",
    "# exp(coef) lower 95% / upper 95%: The confidence interval for the hazard ratio is from 0.69 to 1.11, indicating that PCA_vol1 could potentially have either a protective effect (HR < 1) or no effect (HR  1), and we cannot be sure of its direction of impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "# Stratify by the median of PCA_vol1 for simplicity\n",
    "df['group_vol1'] = (df['PCA_vol1'] > df['PCA_vol1'].median()).astype(int)\n",
    "\n",
    "# Fit Kaplan-Meier estimator\n",
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(df['duration'], event_observed=df['event'], label=\"All Data\")\n",
    "kmf.plot_survival_function()\n",
    "\n",
    "# Stratify by PCA_vol1 (grouping based on median value)\n",
    "kmf.fit(df['duration'][df['group_vol1'] == 0], event_observed=df['event'][df['group_vol1'] == 0], label=\"Low PCA_vol1\")\n",
    "ax = kmf.plot_survival_function()\n",
    "\n",
    "kmf.fit(df['duration'][df['group_vol1'] == 1], event_observed=df['event'][df['group_vol1'] == 1], label=\"High PCA_vol1\")\n",
    "kmf.plot_survival_function(ax=ax)\n",
    "\n",
    "plt.title('Kaplan-Meier Survival Curves Stratified by PCA_vol1')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "# Assuming 'df' is your dataframe with the required columns:\n",
    "# 'duration' (time), 'event' (event indicator), 'PCA_vol1', 'PCA_vol2'\n",
    "\n",
    "# Create a KaplanMeierFitter instance\n",
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "# Stratify by the median of PCA_vol1\n",
    "median_vol1 = df['PCA_vol1'].median()\n",
    "df['group_vol1'] = (df['PCA_vol1'] > median_vol1).astype(int)\n",
    "\n",
    "# Stratify by the median of PCA_vol2\n",
    "median_vol2 = df['PCA_vol2'].median()\n",
    "df['group_vol2'] = (df['PCA_vol2'] > median_vol2).astype(int)\n",
    "\n",
    "# Plot Kaplan-Meier Curves for PCA_vol1 groups\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)  # Left plot for PCA_vol1\n",
    "kmf.fit(df['duration'], event_observed=df['event'], label=\"All Data\")\n",
    "kmf.plot_survival_function()\n",
    "\n",
    "kmf.fit(df['duration'][df['group_vol1'] == 0], event_observed=df['event'][df['group_vol1'] == 0], label=\"Low PCA_vol1\")\n",
    "ax = kmf.plot_survival_function()\n",
    "\n",
    "kmf.fit(df['duration'][df['group_vol1'] == 1], event_observed=df['event'][df['group_vol1'] == 1], label=\"High PCA_vol1\")\n",
    "kmf.plot_survival_function(ax=ax)\n",
    "\n",
    "plt.title('Kaplan-Meier Survival Curves for PCA_vol1')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Kaplan-Meier Curves for PCA_vol2 groups\n",
    "plt.subplot(1, 2, 2)  # Right plot for PCA_vol2\n",
    "kmf.fit(df['duration'], event_observed=df['event'], label=\"All Data\")\n",
    "kmf.plot_survival_function()\n",
    "\n",
    "kmf.fit(df['duration'][df['group_vol2'] == 0], event_observed=df['event'][df['group_vol2'] == 0], label=\"Low PCA_vol2\")\n",
    "ax = kmf.plot_survival_function()\n",
    "\n",
    "kmf.fit(df['duration'][df['group_vol2'] == 1], event_observed=df['event'][df['group_vol2'] == 1], label=\"High PCA_vol2\")\n",
    "kmf.plot_survival_function(ax=ax)\n",
    "\n",
    "plt.title('Kaplan-Meier Survival Curves for PCA_vol2')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "# Assuming 'df' is your dataframe with the required columns:\n",
    "# 'duration' (time), 'event' (event indicator), 'PCA_vol1', 'PCA_vol2'\n",
    "\n",
    "# Create a KaplanMeierFitter instance\n",
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "# Plot Kaplan-Meier Curves for PCA_vol1\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for all data\n",
    "plt.subplot(1, 2, 1)\n",
    "kmf.fit(df['duration'], event_observed=df['event'], label=\"All Data\")\n",
    "kmf.plot_survival_function()\n",
    "plt.title('Kaplan-Meier Survival Curve for PCA_vol1')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Survival Probability')\n",
    "\n",
    "# Plot Kaplan-Meier Curves for PCA_vol2\n",
    "plt.subplot(1, 2, 2)\n",
    "kmf.fit(df['duration'], event_observed=df['event'], label=\"All Data\")\n",
    "kmf.plot_survival_function()\n",
    "plt.title('Kaplan-Meier Survival Curve for PCA_vol2')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Survival Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "# Assuming 'df' is your dataframe with the required columns:\n",
    "# 'duration' (time), 'event' (event indicator), 'PCA_vol1', 'PCA_vol2'\n",
    "\n",
    "# Create a KaplanMeierFitter instance\n",
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "# Create two new groups based on a threshold (e.g., median)\n",
    "# This splits data into two groups: below median and above median\n",
    "median_vol1 = df['PCA_vol1'].median()\n",
    "median_vol2 = df['PCA_vol2'].median()\n",
    "\n",
    "# Group data by whether PCA_vol1 and PCA_vol2 are above or below their respective medians\n",
    "df['group_vol1'] = (df['PCA_vol1'] > median_vol1).astype(int)  # 1 for above median, 0 for below median\n",
    "df['group_vol2'] = (df['PCA_vol2'] > median_vol2).astype(int)  # 1 for above median, 0 for below median\n",
    "\n",
    "# Plot Kaplan-Meier Curves for both PCA_vol1 and PCA_vol2 on the same plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Kaplan-Meier for PCA_vol1 (grouping based on median)\n",
    "kmf.fit(df['duration'][df['group_vol1'] == 0], event_observed=df['event'][df['group_vol1'] == 0], label=\"Low PCA_vol1\")\n",
    "kmf.plot_survival_function()\n",
    "\n",
    "kmf.fit(df['duration'][df['group_vol1'] == 1], event_observed=df['event'][df['group_vol1'] == 1], label=\"High PCA_vol1\")\n",
    "kmf.plot_survival_function()\n",
    "\n",
    "# Kaplan-Meier for PCA_vol2 (grouping based on median)\n",
    "kmf.fit(df['duration'][df['group_vol2'] == 0], event_observed=df['event'][df['group_vol2'] == 0], label=\"Low PCA_vol2\")\n",
    "kmf.plot_survival_function()\n",
    "\n",
    "kmf.fit(df['duration'][df['group_vol2'] == 1], event_observed=df['event'][df['group_vol2'] == 1], label=\"High PCA_vol2\")\n",
    "kmf.plot_survival_function()\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Kaplan-Meier Survival Curves for PCA_vol1 and PCA_vol2')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.legend(title=\"Groups\", loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Plots:\n",
    "# Kaplan-Meier Curves: To visualize survival by feature groups.\n",
    "# Hazard Function Plot: To see the predicted hazard over time for different feature values.\n",
    "# Cumulative Hazard Plot: To see the cumulative hazard over time for different feature values.\n",
    "# Schoenfeld Residuals: To check the proportional hazards assumption.\n",
    "# Forest Plot: To visualize the hazard ratios and their confidence intervals.\n",
    "# Martingale Residuals: To assess the goodness of fit of the Cox model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PCA_vol1'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.linspace(df['PCA_vol1'].min(), df['PCA_vol1'].max(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the baseline hazard\n",
    "baseline_hazard = cox.baseline_hazard_\n",
    "\n",
    "# Calculate the predicted hazards for different values of PCA_vol1\n",
    "import numpy as np\n",
    "\n",
    "# Create a range of PCA_vol1 values (e.g., -2 standard deviations to +2 standard deviations)\n",
    "pca_vol1_range = np.linspace(df['PCA_vol1'].min(), df['PCA_vol1'].max(), 100)\n",
    "predicted_hazards = []\n",
    "\n",
    "for value in pca_vol1_range:\n",
    "    # Create a copy of the covariates with the given value of PCA_vol1\n",
    "    covariates = pd.DataFrame({'PCA_vol1': [value] * len(baseline_hazard)})\n",
    "    # Calculate the hazard for each time point\n",
    "    hazard = np.exp(cox.predict_partial_hazard(covariates))\n",
    "    predicted_hazards.append(hazard)\n",
    "\n",
    "# Plot\n",
    "plt.plot(baseline_hazard.index, np.array(predicted_hazards).flatten(), label=\"PCA_vol1 effect\")\n",
    "plt.title('Predicted Hazard Over Time for Different PCA_vol1 Values')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Hazard')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative hazard\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "# Fit the model\n",
    "cox.fit(df, duration_col='duration', event_col='event')\n",
    "\n",
    "# Plot cumulative hazard for PCA_vol1 ranges (e.g., 1 standard deviation below, mean, and 1 standard deviation above)\n",
    "mean_vol1 = df['PCA_vol1'].mean()\n",
    "std_vol1 = df['PCA_vol1'].std()\n",
    "\n",
    "for value in [mean_vol1 - std_vol1, mean_vol1, mean_vol1 + std_vol1]:\n",
    "    covariates = pd.DataFrame({'PCA_vol1': [value] * len(baseline_hazard)})\n",
    "    cumulative_hazard = cox.predict_cumulative_hazard(covariates)\n",
    "    plt.plot(cumulative_hazard.index, cumulative_hazard.values.flatten(), label=f\"PCA_vol1 = {value:.2f}\")\n",
    "\n",
    "plt.title('Cumulative Hazard Over Time for Different PCA_vol1 Values')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cumulative Hazard')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pca_vol1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vol1[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(data_vol1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vol1_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA plus \n",
    "# average values\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from minisom import MiniSom\n",
    "import pandas as pd  # For handling features\n",
    "\n",
    "# Load the NRRD image using SimpleITK\n",
    "nrrd_path = \"C://Users//Administrator//Desktop//GB_ANALYSIS//GB_ANALYSIS_new//20_mm_final//TMH//gldm/105131012/105131012_original_gldm_SmallDependenceLowGrayLevelEmphasis.nrrd\"\n",
    "import re \n",
    "r1=re.split(string=nrrd_path, pattern='\\/')\n",
    "r2=re.split(string=r1[-1], pattern='_')\n",
    "fname=r2[-1]\n",
    "image = sitk.ReadImage(nrrd_path)\n",
    "image_array = sitk.GetArrayFromImage(image)\n",
    "\n",
    "# Check image dimensions\n",
    "if image_array.ndim != 3:\n",
    "    raise ValueError(\"Image array must be a 3D array\")\n",
    "\n",
    "num_slices = image_array.shape[0]\n",
    "\n",
    "def normalize_slice(slice):\n",
    "    max_val = np.max(slice)\n",
    "    if max_val > 0:\n",
    "        return (slice.astype(np.float32) / max_val).clip(0, 1)\n",
    "    else:\n",
    "        return np.zeros_like(slice, dtype=np.float32)\n",
    "\n",
    "# Define SOM parameters\n",
    "som_width = 2\n",
    "som_height = 2\n",
    "som_depth = 1\n",
    "input_len = image_array.shape[-1] if image_array.ndim == 4 else 1\n",
    "som = MiniSom(x=som_width, y=som_height, input_len=input_len, sigma=1.0, learning_rate=0.5)\n",
    "\n",
    "# Prepare a list to hold pixel values from all slices\n",
    "all_pixels = []\n",
    "\n",
    "# Normalize and gather all pixel values from all slices\n",
    "for i in range(num_slices):\n",
    "    image_slice = image_array[i]\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "    pixels = image_slice_norm.reshape(-1, input_len)\n",
    "    all_pixels.append(pixels)\n",
    "\n",
    "# Concatenate all pixel values into a single array\n",
    "all_pixels = np.vstack(all_pixels)\n",
    "\n",
    "# Train the SOM on the combined pixel values\n",
    "som.random_weights_init(all_pixels)\n",
    "som.train(all_pixels, 5000)\n",
    "\n",
    "# Initialize a dictionary to hold pixel values grouped by cluster labels\n",
    "pixel_clusters = {label: [] for label in range(som_width * som_height * som_depth)}\n",
    "\n",
    "# Map each pixel in each slice to the trained SOM\n",
    "for i in range(num_slices):\n",
    "    image_slice = image_array[i]\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "    labels = np.zeros((image_slice.shape[0], image_slice.shape[1]), dtype=int)\n",
    "\n",
    "    # Map each pixel to the SOM\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            w = som.winner(image_slice_norm[x, y])\n",
    "            labels[x, y] = w[0] * som_height * som_depth + w[1] * som_depth + (i % som_depth)\n",
    "\n",
    "    # Extract pixel values into lists grouped by cluster label\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            cluster_label = labels[x, y]\n",
    "            pixel_value = image_slice[x, y]\n",
    "            pixel_clusters[cluster_label].append(pixel_value)\n",
    "\n",
    "# Calculate the average pixel value for each cluster\n",
    "average_values = []\n",
    "for label in range(som_width * som_height * som_depth):\n",
    "    if pixel_clusters[label]:  # Ensure the cluster is not empty\n",
    "        avg_value = np.mean(pixel_clusters[label])\n",
    "        \n",
    "        average_values.append(avg_value)\n",
    "    else:\n",
    "        average_values.append(np.nan)  # Handle empty clusters\n",
    "\n",
    "# Convert average values to a DataFrame for further analysis\n",
    "features_df = pd.DataFrame({\n",
    "    'Cluster': range(som_width * som_height * som_depth),\n",
    "    'Average_Value': average_values\n",
    "})\n",
    "\n",
    "print(features_df)\n",
    "\n",
    "# You can now use `features_df` for Cox regression or further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average values\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from minisom import MiniSom\n",
    "import pandas as pd  # For handling features\n",
    "\n",
    "# Load the NRRD image using SimpleITK\n",
    "nrrd_path = \"C://Users//Administrator//Desktop//GB_ANALYSIS//GB_ANALYSIS_new//20_mm_final//TMH//gldm/105131012/105131012_original_gldm_SmallDependenceLowGrayLevelEmphasis.nrrd\"\n",
    "import re \n",
    "r1=re.split(string=nrrd_path, pattern='\\/')\n",
    "r2=re.split(string=r1[-1], pattern='_')\n",
    "fname=r2[-1]\n",
    "image = sitk.ReadImage(nrrd_path)\n",
    "image_array = sitk.GetArrayFromImage(image)\n",
    "\n",
    "# Check image dimensions\n",
    "if image_array.ndim != 3:\n",
    "    raise ValueError(\"Image array must be a 3D array\")\n",
    "\n",
    "num_slices = image_array.shape[0]\n",
    "\n",
    "def normalize_slice(slice):\n",
    "    max_val = np.max(slice)\n",
    "    if max_val > 0:\n",
    "        return (slice.astype(np.float32) / max_val).clip(0, 1)\n",
    "    else:\n",
    "        return np.zeros_like(slice, dtype=np.float32)\n",
    "\n",
    "# Define SOM parameters\n",
    "som_width = 2\n",
    "som_height = 2\n",
    "som_depth = 1\n",
    "input_len = image_array.shape[-1] if image_array.ndim == 4 else 1\n",
    "som = MiniSom(x=som_width, y=som_height, input_len=input_len, sigma=1.0, learning_rate=0.5)\n",
    "\n",
    "# Prepare a list to hold pixel values from all slices\n",
    "all_pixels = []\n",
    "\n",
    "# Normalize and gather all pixel values from all slices\n",
    "for i in range(num_slices):\n",
    "    image_slice = image_array[i]\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "    pixels = image_slice_norm.reshape(-1, input_len)\n",
    "    all_pixels.append(pixels)\n",
    "\n",
    "# Concatenate all pixel values into a single array\n",
    "all_pixels = np.vstack(all_pixels)\n",
    "\n",
    "# Train the SOM on the combined pixel values\n",
    "som.random_weights_init(all_pixels)\n",
    "som.train(all_pixels, 5000)\n",
    "\n",
    "# Initialize a dictionary to hold pixel values grouped by cluster labels\n",
    "pixel_clusters = {label: [] for label in range(som_width * som_height * som_depth)}\n",
    "\n",
    "# Map each pixel in each slice to the trained SOM\n",
    "for i in range(num_slices):\n",
    "    image_slice = image_array[i]\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "    labels = np.zeros((image_slice.shape[0], image_slice.shape[1]), dtype=int)\n",
    "\n",
    "    # Map each pixel to the SOM\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            w = som.winner(image_slice_norm[x, y])\n",
    "            labels[x, y] = w[0] * som_height * som_depth + w[1] * som_depth + (i % som_depth)\n",
    "\n",
    "    # Extract pixel values into lists grouped by cluster label\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            cluster_label = labels[x, y]\n",
    "            pixel_value = image_slice[x, y]\n",
    "            pixel_clusters[cluster_label].append(pixel_value)\n",
    "\n",
    "# Calculate the average pixel value for each cluster\n",
    "average_values = []\n",
    "for label in range(som_width * som_height * som_depth):\n",
    "    if pixel_clusters[label]:  # Ensure the cluster is not empty\n",
    "        avg_value = np.mean(pixel_clusters[label])\n",
    "        \n",
    "        average_values.append(avg_value)\n",
    "    else:\n",
    "        average_values.append(np.nan)  # Handle empty clusters\n",
    "\n",
    "# Convert average values to a DataFrame for further analysis\n",
    "features_df = pd.DataFrame({\n",
    "    'Cluster': range(som_width * som_height * som_depth),\n",
    "    'Average_Value': average_values\n",
    "})\n",
    "\n",
    "print(features_df)\n",
    "\n",
    "# You can now use `features_df` for Cox regression or further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from minisom import MiniSom\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the NRRD image using SimpleITK\n",
    "nrrd_path = \"C://Users//Administrator//Desktop//GB_ANALYSIS//GB_ANALYSIS_new//20_mm_final//TMH//gldm/105131012/105131012_original_gldm_HighGrayLevelEmphasis.nrrd\"\n",
    "image = sitk.ReadImage(nrrd_path)\n",
    "image_array = sitk.GetArrayFromImage(image)\n",
    "\n",
    "# Check image dimensions\n",
    "if image_array.ndim != 3:\n",
    "    raise ValueError(\"Image array must be a 3D array\")\n",
    "\n",
    "num_slices = image_array.shape[0]\n",
    "\n",
    "def normalize_slice(slice):\n",
    "    max_val = np.max(slice)\n",
    "    if max_val > 0:\n",
    "        return (slice.astype(np.float32) / max_val).clip(0, 1)\n",
    "    else:\n",
    "        return np.zeros_like(slice, dtype=np.float32)\n",
    "\n",
    "# Define SOM parameters\n",
    "som_width = 2\n",
    "som_height = 2\n",
    "som_depth = 1\n",
    "input_len = image_array.shape[-1] if image_array.ndim == 4 else 1\n",
    "som = MiniSom(x=som_width, y=som_height, input_len=input_len, sigma=1.0, learning_rate=0.5)\n",
    "\n",
    "# Initialize a dictionary to hold pixel values grouped by cluster labels\n",
    "pixel_clusters = {label: [] for label in range(som_width * som_height * som_depth)}\n",
    "\n",
    "# Map each pixel in each slice to the trained SOM\n",
    "for i in range(num_slices):\n",
    "    image_slice = image_array[i]\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "    labels = np.zeros((image_slice.shape[0], image_slice.shape[1]), dtype=int)\n",
    "\n",
    "    # Map each pixel to the SOM\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            w = som.winner(image_slice_norm[x, y])\n",
    "            labels[x, y] = w[0] * som_height * som_depth + w[1] * som_depth + (i % som_depth)\n",
    "\n",
    "    # Extract pixel values into lists grouped by cluster label\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            cluster_label = labels[x, y]\n",
    "            pixel_value = image_slice[x, y]\n",
    "            pixel_clusters[cluster_label].append(pixel_value)\n",
    "\n",
    "# Calculate sum, product, and PCA for each cluster\n",
    "features = []\n",
    "for label in range(som_width * som_height * som_depth):\n",
    "    if pixel_clusters[label]:  # Ensure the cluster is not empty\n",
    "        pixel_values = np.array(pixel_clusters[label])\n",
    "        \n",
    "        # Calculate sum\n",
    "        cluster_sum = np.sum(pixel_values)\n",
    "        \n",
    "        # Calculate product\n",
    "        cluster_product = np.prod(pixel_values) if len(pixel_values) > 0 else 0\n",
    "        \n",
    "        # Calculate PCA and get the first principal component\n",
    "        pca = PCA(n_components=1)\n",
    "        pixel_values_reshaped = pixel_values.reshape(-1, 1)  # Reshape for PCA\n",
    "        pca.fit(pixel_values_reshaped)\n",
    "        pc1 = pca.transform(pixel_values_reshaped)\n",
    "\n",
    "        # Handle the case where pc1 might be a single scalar\n",
    "        first_pc_value = pc1[0, 0] if pc1.shape[0] > 0 else np.nan\n",
    "        \n",
    "        features.append({\n",
    "            'Cluster': label,\n",
    "            'Sum': cluster_sum,\n",
    "            'Product': cluster_product,\n",
    "            'PC1': first_pc_value\n",
    "        })\n",
    "    else:\n",
    "        features.append({\n",
    "            'Cluster': label,\n",
    "            'Sum': np.nan,\n",
    "            'Product': np.nan,\n",
    "            'PC1': np.nan\n",
    "        })\n",
    "\n",
    "# Convert features to a DataFrame for further analysis\n",
    "features_df = pd.DataFrame(features)\n",
    "print(features_df)\n",
    "\n",
    "# You can now use `features_df` for Cox regression or further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from minisom import MiniSom\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the NRRD image using SimpleITK\n",
    "nrrd_path = \"C://Users//Administrator//Desktop//GB_ANALYSIS//GB_ANALYSIS_new//20_mm_final//TMH//gldm/105131012/105131012_original_gldm_HighGrayLevelEmphasis.nrrd\"\n",
    "image = sitk.ReadImage(nrrd_path)\n",
    "image_array = sitk.GetArrayFromImage(image)\n",
    "\n",
    "# Check image dimensions\n",
    "if image_array.ndim != 3:\n",
    "    raise ValueError(\"Image array must be a 3D array\")\n",
    "\n",
    "num_slices = image_array.shape[0]\n",
    "\n",
    "def normalize_slice(slice):\n",
    "    max_val = np.max(slice)\n",
    "    if max_val > 0:\n",
    "        return (slice.astype(np.float32) / max_val).clip(0, 1)\n",
    "    else:\n",
    "        return np.zeros_like(slice, dtype=np.float32)\n",
    "\n",
    "# Define SOM parameters\n",
    "som_width = 2\n",
    "som_height = 2\n",
    "som_depth = 1\n",
    "input_len = image_array.shape[-1] if image_array.ndim == 4 else 1\n",
    "som = MiniSom(x=som_width, y=som_height, input_len=input_len, sigma=1.0, learning_rate=0.5)\n",
    "\n",
    "# Initialize a dictionary to hold pixel values grouped by cluster labels\n",
    "pixel_clusters = {label: [] for label in range(som_width * som_height * som_depth)}\n",
    "\n",
    "# Map each pixel in each slice to the trained SOM, excluding the first and last slices\n",
    "for i in range(1, num_slices - 1):  # Start from 1 and end at num_slices - 1\n",
    "    image_slice = image_array[i]\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "    labels = np.zeros((image_slice.shape[0], image_slice.shape[1]), dtype=int)\n",
    "\n",
    "    # Map each pixel to the SOM\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            w = som.winner(image_slice_norm[x, y])\n",
    "            labels[x, y] = w[0] * som_height * som_depth + w[1] * som_depth + (i % som_depth)\n",
    "\n",
    "    # Extract pixel values into lists grouped by cluster label\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            cluster_label = labels[x, y]\n",
    "            pixel_value = image_slice[x, y]\n",
    "            pixel_clusters[cluster_label].append(pixel_value)\n",
    "\n",
    "# Calculate sum, product, and PCA for each cluster\n",
    "features = []\n",
    "for label in range(som_width * som_height * som_depth):\n",
    "    if pixel_clusters[label]:  # Ensure the cluster is not empty\n",
    "        pixel_values = np.array(pixel_clusters[label])\n",
    "        print(\"pixel_values:\",pixel_values[:])\n",
    "        # Calculate sum\n",
    "        cluster_sum = np.sum(pixel_values)\n",
    "        \n",
    "        # Calculate product\n",
    "        cluster_product = np.prod(pixel_values) if len(pixel_values) > 0 else 0\n",
    "        \n",
    "        # Calculate PCA and get the first principal component\n",
    "        pca = PCA(n_components=1)\n",
    "        pixel_values_reshaped = pixel_values.reshape(-1, 1)  # Reshape for PCA\n",
    "        pca.fit(pixel_values_reshaped)\n",
    "        pc1 = pca.transform(pixel_values_reshaped)\n",
    "        print('pc1',pc1)\n",
    "        # Handle the case where pc1 might be a single scalar\n",
    "        first_pc_value = pc1[0, 0] if pc1.shape[0] > 0 else np.nan\n",
    "        \n",
    "        features.append({\n",
    "            'Cluster': label,\n",
    "            'Sum': cluster_sum,\n",
    "            'Product': cluster_product,\n",
    "            'PC1': first_pc_value\n",
    "        })\n",
    "    else:\n",
    "        features.append({\n",
    "            'Cluster': label,\n",
    "            'Sum': np.nan,\n",
    "            'Product': np.nan,\n",
    "            'PC1': np.nan\n",
    "        })\n",
    "\n",
    "# Convert features to a DataFrame for further analysis\n",
    "features_df = pd.DataFrame(features)\n",
    "print(features_df)\n",
    "\n",
    "# You can now use `features_df` for Cox regression or further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from minisom import MiniSom\n",
    "\n",
    "# Load the NRRD image using SimpleITK\n",
    "nrrd_path = \"C://Users//Administrator//Desktop//GB_ANALYSIS//GB_ANALYSIS_new//20_mm_final//TMH//gldm/105131012/105131012_original_gldm_HighGrayLevelEmphasis.nrrd\"\n",
    "image = sitk.ReadImage(nrrd_path)\n",
    "image_array = sitk.GetArrayFromImage(image)\n",
    "\n",
    "# Check image dimensions\n",
    "if image_array.ndim not in (3, 4):\n",
    "    raise ValueError(\"Image array must be a 3D or 4D array\")\n",
    "\n",
    "num_slices = image_array.shape[0] if image_array.ndim == 3 else 1\n",
    "\n",
    "def normalize_slice(slice):\n",
    "    max_val = np.max(slice)\n",
    "    if max_val > 0:\n",
    "        return (slice.astype(np.float32) / max_val).clip(0, 1)\n",
    "    else:\n",
    "        return np.zeros_like(slice, dtype=np.float32)\n",
    "\n",
    "# Define SOM parameters (increased size)\n",
    "som_width = 4  # Increased width\n",
    "som_height =3  # Increased height\n",
    "input_len = image_array.shape[-1] if image_array.ndim == 4 else 1\n",
    "som = MiniSom(x=som_width, y=som_height, input_len=input_len, sigma=1.0, learning_rate=0.5)\n",
    "\n",
    "# Collect all pixels from all slices for training\n",
    "all_pixels = []\n",
    "\n",
    "for i in range(num_slices):\n",
    "    image_slice = image_array[i]\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "    pixels = image_slice_norm.reshape(-1, input_len)\n",
    "    all_pixels.append(pixels)\n",
    "\n",
    "# Concatenate all pixel arrays into one\n",
    "all_pixels = np.vstack(all_pixels)\n",
    "\n",
    "# Train the SOM on all pixels\n",
    "som.random_weights_init(all_pixels)\n",
    "som.train(all_pixels, 2000)  # Increased iterations\n",
    "\n",
    "# Create a 3D array to hold cluster labels\n",
    "labels_3d = np.zeros(image_array.shape, dtype=int)\n",
    "\n",
    "# Map each voxel to the SOM and assign a label\n",
    "for i in range(num_slices):\n",
    "    image_slice = image_array[i]\n",
    "    image_slice_norm = normalize_slice(image_slice)\n",
    "    pixels = image_slice_norm.reshape(-1, input_len)\n",
    "    \n",
    "    labels = np.zeros((image_slice.shape[0], image_slice.shape[1]), dtype=int)\n",
    "    for x in range(image_slice.shape[0]):\n",
    "        for y in range(image_slice.shape[1]):\n",
    "            w = som.winner(image_slice_norm[x, y])\n",
    "            labels[x, y] = w[0] * som_width + w[1]\n",
    "\n",
    "    labels_3d[i] = labels\n",
    "\n",
    "    # Visualize the clusters as images\n",
    "    cluster_images = []\n",
    "    for label in range(som_width * som_height):\n",
    "        cluster_image = np.zeros_like(image_slice, dtype=np.float32)\n",
    "        cluster_image[labels == label] = image_slice[labels == label]\n",
    "        cluster_images.append(cluster_image)\n",
    "\n",
    "    # Plot the original and cluster images\n",
    "    fig, axes = plt.subplots(1, som_width * som_height + 1, figsize=(15, 5))\n",
    "\n",
    "    # Plot the original image\n",
    "    axes[0].imshow(image_slice_norm, cmap='gray')\n",
    "    axes[0].set_title(f'Original Image (Slice {i})')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Plot each cluster image\n",
    "    for j, cluster_image in enumerate(cluster_images):\n",
    "        axes[j + 1].imshow(cluster_image, cmap='gray')\n",
    "        axes[j + 1].set_title(f'Cluster {j}')\n",
    "        axes[j + 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'C:/Users/Administrator/Desktop/gb_voxel_radiomics_analysis/example_images/gb_test.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import cv2\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = 'C:/Users/Administrator/Desktop/gb_voxel_radiomics_analysis/example_images/gb_test.jpg'  # Replace with your image path\n",
    "image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "image = cv2.resize(image, (128, 128))  # Resize for faster processing\n",
    "image_normalized = image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Prepare the data for the autoencoder\n",
    "# Create patches by reshaping the image\n",
    "image_patches = image_normalized.reshape(-1, 128 * 128)  # Shape: (num_pixels, 16384)\n",
    "\n",
    "# Build the autoencoder\n",
    "input_dim = image_patches.shape[1]  # 16384\n",
    "encoding_dim = 64  # Dimension of the encoded representation\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = keras.Sequential([\n",
    "    layers.Input(shape=(input_dim,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(encoding_dim, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(input_dim, activation='sigmoid')\n",
    "])\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(image_patches, image_patches, epochs=100, batch_size=32, shuffle=True)  # Batch size adjusted\n",
    "\n",
    "# Extract the encoder part\n",
    "encoder = keras.Model(inputs=autoencoder.input, outputs=autoencoder.layers[2].output)\n",
    "\n",
    "# Get the encoded representations\n",
    "encoded_data = encoder.predict(image_patches)\n",
    "\n",
    "# Plot the encoded data\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Flatten the encoded data for plotting\n",
    "encoded_flat = encoded_data.flatten()\n",
    "\n",
    "# Create x values as pixel indices\n",
    "x_values = np.arange(len(encoded_flat))\n",
    "\n",
    "plt.scatter(x_values, encoded_flat, s=5, c='blue', alpha=0.5)\n",
    "plt.title('Encoded Data from Autoencoder')\n",
    "plt.xlabel('Pixel Index')\n",
    "plt.ylabel('Encoded Value')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Apply K-means clustering\n",
    "num_clusters = 4  # You can adjust this\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(encoded_data)\n",
    "\n",
    "# Get cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Reshape labels to match the original image shape\n",
    "labels_image = cluster_labels.reshape(128, 128)\n",
    "\n",
    "# Visualize the original image and the clustered segments\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot the original image\n",
    "axes[0].imshow(image_normalized, cmap='gray')\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Plot the clustered image\n",
    "axes[1].imshow(labels_image, cmap='jet', alpha=0.5)\n",
    "axes[1].set_title('Clustered Segmentation')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Optional: Show the clusters in different colors\n",
    "segmented_image = np.zeros_like(image_normalized)\n",
    "for cluster in range(num_clusters):\n",
    "    segmented_image[labels_image == cluster] = cluster / num_clusters\n",
    "\n",
    "axes[2].imshow(segmented_image, cmap   'jet')\n",
    "axes[2].set_title('Segmented Image')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame for your features\n",
    "data = pd.DataFrame({\n",
    "    'feature_1': cluster1_values,\n",
    "    'feature_2': cluster2_values,\n",
    "    # Add other features as needed\n",
    "    'duration': durations,  # Time-to-event or duration\n",
    "    'event': events         # Event occurred (1) or cen7jmjjjjjjjjjjjj9jsored (0)\n",
    "})\n",
    "\n",
    "# Fit the Cox model\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(data, duration_col='duration', event_col='event')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv('C:/Users/Administrator/Desktop/GB_ANALYSIS/GB_ANALYSIS_new/gb_voxel_radiomics_survival_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pca_df['death_disease_specific']=df['death_disease_specific']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pca_df['death_interval_days']=df['death_interval_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pca_df['death_disease_specific'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmh_pca=final_pca_df[['cluster1','cluster2','cluster3','cluster4','death_disease_specific','death_interval_days']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmh_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "import pandas as pd\n",
    "\n",
    "# Fit the Cox model\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(df_tmh_pca, duration_col='death_interval_days', event_col='death_disease_specific')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the summary of the fitted model\n",
    "cph.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster2 is statistically significant(p-value < 0.05).\n",
    "cluster1 and cluster3 are not significant (p-value > 0.05).\n",
    "cluster4 is marginally significant, with a small negative effect, but the p-value (0.052570) is just above the threshold of 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"C-index:\", c_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph.plot()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph.plot_covariate_groups(, values=[value1, value2], cmap='coolwarm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cph.check_assumptions(df_tmh_pca, p_value_threshold=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the survival function for specific values of the covariates\n",
    "cph.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the values for which you want to estimate survival\n",
    "# Example: using the mean or median of covariates\n",
    "mean_covariates = df_tmh.mean().to_frame().T  # Replace with actual covariate values as needed\n",
    "\n",
    "# Estimate the survival function\n",
    "survival_function = cph.predict_survival_function(mean_covariates)\n",
    "\n",
    "# Plot the survival function\n",
    "survival_function.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefficients and p-values\n",
    "coefficients = cph.params_\n",
    "\n",
    "\n",
    "print(\"Coefficients:\\n\", coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
